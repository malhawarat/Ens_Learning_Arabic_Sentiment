{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RN-D6WF852bu",
        "y_RnzIFc5-Iy",
        "8-rSfOh46IGW"
      ],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arabic Sentiment Analysis with Ensemble Learning\n",
        "\n",
        "HOW TO USE:\n",
        "1. Runtime > Change runtime type > GPU (T4 or better)\n",
        "2. Run cells in order\n",
        "3. Results will be saved to Google Drive\n"
      ],
      "metadata": {
        "id": "Pip0OKjtw4wM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Setup and Installation"
      ],
      "metadata": {
        "id": "kj8YwnWr5Wc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db8U2sn3vlzN",
        "outputId": "e00062c6-a3f8-441b-bdb1-4c24347830f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ARABIC SENTIMENT ANALYSIS - GOOGLE COLAB SETUP\n",
            "======================================================================\n",
            "\n",
            "\ud83d\udce6 Installing required packages...\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\ud83d\udcbe Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\u2713 Project directory created at: /content/drive/MyDrive/arabic_sentiment_analysis\n",
            "\n",
            "\u26a0\ufe0f  IMPORTANT: Upload 'balanced-reviews.csv' to:\n",
            "   /content/drive/MyDrive/arabic_sentiment_analysis/data/balanced-reviews.csv\n",
            "\n",
            "You can download the dataset from:\n",
            "   https://github.com/elnagara/HARD-Arabic-Dataset\n",
            "\n",
            "\u2713 GPU Available: NVIDIA A100-SXM4-80GB\n",
            "   Memory: 85.17 GB\n",
            "\n",
            "======================================================================\n",
            "Setup complete! You can now run the next cells.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ARABIC SENTIMENT ANALYSIS - GOOGLE COLAB SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Install required packages\n",
        "print(\"\\n\ud83d\udce6 Installing required packages...\")\n",
        "!pip install -q transformers datasets torch scikit-learn pandas numpy tqdm matplotlib seaborn\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\n\ud83d\udcbe Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create project directories in Google Drive\n",
        "import os\n",
        "project_path = '/content/drive/MyDrive/arabic_sentiment_analysis'\n",
        "os.makedirs(f'{project_path}/data', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/saved_models', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/results', exist_ok=True)\n",
        "os.makedirs(f'{project_path}/figures', exist_ok=True)\n",
        "\n",
        "print(f\"\u2713 Project directory created at: {project_path}\")\n",
        "print(\"\\n\u26a0\ufe0f  IMPORTANT: Upload 'balanced-reviews.csv' to:\")\n",
        "print(f\"   {project_path}/data/balanced-reviews.csv\")\n",
        "print(\"\\nYou can download the dataset from:\")\n",
        "print(\"   https://github.com/elnagara/HARD-Arabic-Dataset\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n\u2713 GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  WARNING: No GPU detected. Training will be very slow!\")\n",
        "    print(\"   Go to: Runtime > Change runtime type > GPU\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Setup complete! You can now run the next cells.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Configuration"
      ],
      "metadata": {
        "id": "FJJLqVNZ5eVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Central configuration for all experiments\"\"\"\n",
        "\n",
        "    # Paths (Google Drive)\n",
        "    PROJECT_PATH = '/content/drive/MyDrive/arabic_sentiment_analysis'\n",
        "    DATA_DIR = f'{PROJECT_PATH}/data'\n",
        "    MODEL_DIR = f'{PROJECT_PATH}/saved_models'\n",
        "    RESULTS_DIR = f'{PROJECT_PATH}/results'\n",
        "    FIGURES_DIR = f'{PROJECT_PATH}/figures'\n",
        "\n",
        "    # Dataset settings\n",
        "    DATASET_FILE = 'balanced-reviews.csv'\n",
        "    TRAIN_RATIO = 0.8\n",
        "    VAL_RATIO = 0.1\n",
        "    TEST_RATIO = 0.1\n",
        "\n",
        "    # Model configurations\n",
        "    MODELS = {\n",
        "        'arabert': 'aubmindlab/bert-base-arabert',\n",
        "        'marbert': 'UBC-NLP/MARBERT',\n",
        "        'xlm-roberta': 'xlm-roberta-base',\n",
        "        'camelbert': 'CAMeL-Lab/bert-base-arabic-camelbert-msa'\n",
        "    }\n",
        "\n",
        "    # Training hyperparameters\n",
        "    LEARNING_RATE = 2e-5\n",
        "    BATCH_SIZE = 16  # Reduced for Colab GPU\n",
        "    NUM_EPOCHS = 3\n",
        "    MAX_LENGTH = 512\n",
        "    WARMUP_RATIO = 0.1\n",
        "    WEIGHT_DECAY = 0.01\n",
        "\n",
        "    # Early stopping\n",
        "    PATIENCE = 3\n",
        "\n",
        "    # Reproducibility - Use fewer seeds for faster runtime in Colab\n",
        "    RANDOM_SEEDS = [42, 123, 456, 789, 2024]  # Reduced from 5 to 2 for faster execution\n",
        "\n",
        "    # Device\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Number of labels\n",
        "    NUM_LABELS = 2\n",
        "\n",
        "config = Config()\n",
        "print(\"\u2713 Configuration loaded\")\n",
        "print(f\"  Device: {config.DEVICE}\")\n",
        "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Random Seeds: {config.RANDOM_SEEDS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEU2EzxFzGi5",
        "outputId": "98eeee4d-94a6-46a2-cf58-ad3c4c1e45c9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Configuration loaded\n",
            "  Device: cuda\n",
            "  Batch Size: 16\n",
            "  Random Seeds: [123]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 - Data Preprocessing"
      ],
      "metadata": {
        "id": "3OGOrR1n5oPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class ArabicPreprocessor:\n",
        "    \"\"\"Arabic text preprocessing utilities\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_arabic(text):\n",
        "        \"\"\"Normalize Arabic text\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Normalize different forms of Alef\n",
        "        text = re.sub('[\u0625\u0623\u0622\u0627]', '\u0627', text)\n",
        "        text = re.sub('\u0649', '\u064a', text)\n",
        "        text = re.sub('\u0629', '\u0647', text)\n",
        "\n",
        "        # Remove diacritics\n",
        "        arabic_diacritics = re.compile(\"\"\"\n",
        "                                 \u0651    | # Tashdid\n",
        "                                 \u064e    | # Fatha\n",
        "                                 \u064b    | # Tanwin Fath\n",
        "                                 \u064f    | # Damma\n",
        "                                 \u064c    | # Tanwin Damm\n",
        "                                 \u0650    | # Kasra\n",
        "                                 \u064d    | # Tanwin Kasr\n",
        "                                 \u0652    | # Sukun\n",
        "                                 \u0640     # Tatwil/Kashida\n",
        "                             \"\"\", re.VERBOSE)\n",
        "        text = re.sub(arabic_diacritics, '', text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_text(text):\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(text):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        text = ArabicPreprocessor.normalize_arabic(text)\n",
        "        text = ArabicPreprocessor.clean_text(text)\n",
        "        return text\n",
        "\n",
        "\n",
        "class HARDDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for HARD\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "class DataManager:\n",
        "    \"\"\"Manage dataset loading, preprocessing, and splitting\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.preprocessor = ArabicPreprocessor()\n",
        "\n",
        "    def load_hard_dataset(self, file_path):\n",
        "        \"\"\"Load HARD dataset from CSV/TSV\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, sep=',', encoding='utf-8')\n",
        "        except:\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, encoding='utf-8')\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading dataset: {e}\")\n",
        "                print(\"Please ensure the file is in the correct format.\")\n",
        "                return None\n",
        "\n",
        "        # Standardize column names\n",
        "        if 'review' in df.columns:\n",
        "          df = df.rename(columns={'review': 'text'})\n",
        "        if 'rating' in df.columns:\n",
        "          df = df.rename(columns={'rating': 'label'})\n",
        "\n",
        "        print(\"Preprocessing texts...\")\n",
        "        df['text'] = df['text'].apply(self.preprocessor.preprocess)\n",
        "\n",
        "        # Remove empty texts\n",
        "        df = df[df['text'].str.len() > 0]\n",
        "\n",
        "        # Ensure labels are 0/1\n",
        "        label_map = {1: 0, 2: 0, 4: 1, 5: 1}\n",
        "        df['label'] = df['label'].map(label_map)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def split_data(self, df, seed=123):\n",
        "        \"\"\"Split data into train/val/test\"\"\"\n",
        "        train_val, test = train_test_split(\n",
        "            df,\n",
        "            test_size=self.config.TEST_RATIO,\n",
        "            random_state=seed,\n",
        "            stratify=df['label']\n",
        "        )\n",
        "\n",
        "        val_ratio_adjusted = self.config.VAL_RATIO / (self.config.TRAIN_RATIO + self.config.VAL_RATIO)\n",
        "        train, val = train_test_split(\n",
        "            train_val,\n",
        "            test_size=val_ratio_adjusted,\n",
        "            random_state=seed,\n",
        "            stratify=train_val['label']\n",
        "        )\n",
        "\n",
        "        print(f\"Data split (seed={seed}):\")\n",
        "        print(f\"  Train: {len(train)} samples\")\n",
        "        print(f\"  Val:   {len(val)} samples\")\n",
        "        print(f\"  Test:  {len(test)} samples\")\n",
        "\n",
        "        return train, val, test\n",
        "\n",
        "    def create_dataloaders(self, train_df, val_df, test_df, tokenizer, batch_size):\n",
        "        \"\"\"Create PyTorch DataLoaders\"\"\"\n",
        "        train_dataset = HARDDataset(\n",
        "            train_df['text'].values,\n",
        "            train_df['label'].values,\n",
        "            tokenizer,\n",
        "            self.config.MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        val_dataset = HARDDataset(\n",
        "            val_df['text'].values,\n",
        "            val_df['label'].values,\n",
        "            tokenizer,\n",
        "            self.config.MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        test_dataset = HARDDataset(\n",
        "            test_df['text'].values,\n",
        "            test_df['label'].values,\n",
        "            tokenizer,\n",
        "            self.config.MAX_LENGTH\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        return train_loader, val_loader, test_loader\n",
        "\n",
        "print(\"\u2713 Data preprocessing classes defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLErnZtJzkVr",
        "outputId": "878d5417-9e74-4e07-d6a9-65bd675a6fc7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Data preprocessing classes defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 - Model Training"
      ],
      "metadata": {
        "id": "RN-D6WF852bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "class SentimentClassifier:\n",
        "    \"\"\"Wrapper for fine-tuning BERT-based models\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, config, device):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_name,\n",
        "            num_labels=config.NUM_LABELS\n",
        "        )\n",
        "        self.model.to(device)\n",
        "\n",
        "        self.best_val_acc = 0\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def train_epoch(self, train_loader, optimizer, scheduler):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc='Training')\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "            labels = batch['labels'].to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{correct/total:.4f}'\n",
        "            })\n",
        "\n",
        "        return total_loss / len(train_loader), correct / total\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        \"\"\"Evaluate on validation/test set\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Evaluating'):\n",
        "                input_ids = batch['input_ids'].to(self.device)\n",
        "                attention_mask = batch['attention_mask'].to(self.device)\n",
        "                labels = batch['labels'].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                probs = torch.softmax(logits, dim=1)\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        accuracy = correct / total\n",
        "        avg_loss = total_loss / len(val_loader)\n",
        "\n",
        "        return avg_loss, accuracy, all_preds, all_labels, all_probs\n",
        "\n",
        "    def train(self, train_loader, val_loader, save_path):\n",
        "        \"\"\"Complete training loop with early stopping\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Training {self.model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.LEARNING_RATE,\n",
        "            weight_decay=self.config.WEIGHT_DECAY\n",
        "        )\n",
        "\n",
        "        num_training_steps = len(train_loader) * self.config.NUM_EPOCHS\n",
        "        num_warmup_steps = int(num_training_steps * self.config.WARMUP_RATIO)\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config.NUM_EPOCHS):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.config.NUM_EPOCHS}\")\n",
        "\n",
        "            train_loss, train_acc = self.train_epoch(train_loader, optimizer, scheduler)\n",
        "            val_loss, val_acc, _, _, _ = self.evaluate(val_loader)\n",
        "\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "            if val_acc > self.best_val_acc:\n",
        "                self.best_val_acc = val_acc\n",
        "                self.patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), save_path)\n",
        "                print(f\"\u2713 Best model saved! Val Acc: {val_acc:.4f}\")\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                print(f\"No improvement ({self.patience_counter}/{self.config.PATIENCE})\")\n",
        "\n",
        "                if self.patience_counter >= self.config.PATIENCE:\n",
        "                    print(\"Early stopping triggered!\")\n",
        "                    break\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"\\nTraining completed in {training_time/3600:.2f} hours\")\n",
        "        print(f\"Best validation accuracy: {self.best_val_acc:.4f}\")\n",
        "\n",
        "        self.model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "        return self.best_val_acc, training_time\n",
        "\n",
        "    def predict(self, test_loader):\n",
        "        \"\"\"Get predictions on test set\"\"\"\n",
        "        _, accuracy, preds, labels, probs = self.evaluate(test_loader)\n",
        "        return accuracy, preds, labels, probs\n",
        "\n",
        "print(\"\u2713 Model training classes defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tew1iPIWzuBd",
        "outputId": "18357835-9537-4fd1-c8db-618889a33025"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Model training classes defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 - Ensemble Methods"
      ],
      "metadata": {
        "id": "y_RnzIFc5-Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "class EnsembleMethods:\n",
        "    \"\"\"Implementation of various ensemble strategies\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_voting(predictions_probs):\n",
        "        \"\"\"Soft voting: Average probabilities from all models\"\"\"\n",
        "        avg_probs = np.mean(predictions_probs, axis=0)\n",
        "        final_preds = np.argmax(avg_probs, axis=1)\n",
        "        return final_preds, avg_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def hard_voting(predictions):\n",
        "        \"\"\"Hard voting: Majority vote on predicted classes\"\"\"\n",
        "        stacked = np.column_stack(predictions)\n",
        "        final_preds = []\n",
        "        for row in stacked:\n",
        "            counts = np.bincount(row)\n",
        "            final_preds.append(np.argmax(counts))\n",
        "        return np.array(final_preds)\n",
        "\n",
        "    @staticmethod\n",
        "    def weighted_voting(predictions_probs, weights):\n",
        "        \"\"\"Weighted voting: Weight models by validation performance\"\"\"\n",
        "        # Convert weights to numpy array and normalize\n",
        "        weights = np.array(weights, dtype=np.float64)\n",
        "        weights = weights / np.sum(weights)\n",
        "\n",
        "        # Initialize weighted probabilities\n",
        "        weighted_probs = np.zeros_like(predictions_probs[0], dtype=np.float64)\n",
        "\n",
        "        # Apply weighted sum\n",
        "        for i, probs in enumerate(predictions_probs):\n",
        "            probs = np.array(probs, dtype=np.float64)  # Ensure probs is numpy array\n",
        "            weighted_probs += weights[i] * probs\n",
        "\n",
        "        final_preds = np.argmax(weighted_probs, axis=1)\n",
        "        return final_preds, weighted_probs\n",
        "\n",
        "    @staticmethod\n",
        "    def stacking(train_probs, train_labels, test_probs):\n",
        "        \"\"\"Stacking: Train meta-classifier on base model predictions\"\"\"\n",
        "        X_train = np.hstack([probs for probs in train_probs])\n",
        "        X_test = np.hstack([probs for probs in test_probs])\n",
        "\n",
        "        meta_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "        meta_clf.fit(X_train, train_labels)\n",
        "\n",
        "        final_preds = meta_clf.predict(X_test)\n",
        "        final_probs = meta_clf.predict_proba(X_test)\n",
        "\n",
        "        return final_preds, final_probs, meta_clf\n",
        "\n",
        "print(\"\u2713 Ensemble methods defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQnQzPM105If",
        "outputId": "9b1506d6-8cbc-4ff3-f6d1-9de2b3f508a0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Ensemble methods defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 - Evaluation Utilities"
      ],
      "metadata": {
        "id": "8-rSfOh46IGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    confusion_matrix\n",
        ")\n",
        "import json\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"Evaluation utilities\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_metrics(y_true, y_pred):\n",
        "        \"\"\"Calculate all evaluation metrics\"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='macro'\n",
        "        )\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'confusion_matrix': cm.tolist()\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def print_results(name, metrics):\n",
        "        \"\"\"Pretty print results\"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"{name} Results\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "        print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "        print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(np.array(metrics['confusion_matrix']))\n",
        "\n",
        "    @staticmethod\n",
        "    def save_results(results, filepath):\n",
        "        \"\"\"Save results to JSON\"\"\"\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Results saved to {filepath}\")\n",
        "\n",
        "print(\"\u2713 Evaluation utilities defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbdUEwQk098D",
        "outputId": "cc57c2e4-e60e-4dfc-b93e-16601e14e06b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2713 Evaluation utilities defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 - Main Execution"
      ],
      "metadata": {
        "id": "OfeLG8fN6LxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution pipeline with integrated statistical, error, and runtime analysis\"\"\"\n",
        "    \n",
        "    # Initialize runtime tracking\n",
        "    runtime_tracker.start_experiment()\n",
        "    \n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"ARABIC SENTIMENT ANALYSIS - STARTING EXPERIMENTS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Step 1: Loading dataset...\")\n",
        "    data_manager = DataManager(config)\n",
        "\n",
        "    dataset_path = os.path.join(config.DATA_DIR, config.DATASET_FILE)\n",
        "\n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"\u274c ERROR: Dataset not found at {dataset_path}\")\n",
        "        print(\"\\nPlease upload 'balanced-reviews.csv' to:\")\n",
        "        print(f\"   {config.DATA_DIR}/\")\n",
        "        print(\"\\nDownload from: https://github.com/elnagara/HARD-Arabic-Dataset\")\n",
        "        return\n",
        "\n",
        "    df = data_manager.load_hard_dataset(dataset_path)\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    print(f\"\u2713 Loaded {len(df)} reviews\")\n",
        "\n",
        "    # Store results\n",
        "    all_results = {}\n",
        "    \n",
        "    # Store test data for error analysis\n",
        "    stored_test_data = {}\n",
        "\n",
        "    # Run experiments\n",
        "    for seed_idx, seed in enumerate(config.RANDOM_SEEDS):\n",
        "        print(f\"\\n{'#'*70}\")\n",
        "        print(f\"EXPERIMENT {seed_idx + 1}/{len(config.RANDOM_SEEDS)} - Seed: {seed}\")\n",
        "        print(f\"{'#'*70}\\n\")\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        # Track runtime for this seed\n",
        "        runtime_tracker.start_seed(seed)\n",
        "\n",
        "        # Split data\n",
        "        train_df, val_df, test_df = data_manager.split_data(df, seed=seed)\n",
        "\n",
        "        # Store predictions\n",
        "        model_predictions = {}\n",
        "        model_probs = {}\n",
        "        model_val_accuracies = {}\n",
        "\n",
        "        # Train individual models\n",
        "        for model_key, model_name in config.MODELS.items():\n",
        "            print(f\"\\n{'='*70}\")\n",
        "            print(f\"Training {model_key}: {model_name}\")\n",
        "            print(f\"{'='*70}\")\n",
        "\n",
        "            # Initialize model\n",
        "            classifier = SentimentClassifier(model_name, config, config.DEVICE)\n",
        "\n",
        "            # Create dataloaders\n",
        "            train_loader, val_loader, test_loader = data_manager.create_dataloaders(\n",
        "                train_df, val_df, test_df,\n",
        "                classifier.tokenizer,\n",
        "                config.BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            # Train\n",
        "            model_path = os.path.join(config.MODEL_DIR, f'{model_key}_seed{seed}.pt')\n",
        "            val_acc, train_time = classifier.train(train_loader, val_loader, model_path)\n",
        "            \n",
        "            # Log training time\n",
        "            runtime_tracker.log_runtime(seed, model_key, train_time)\n",
        "\n",
        "            # Evaluate\n",
        "            test_acc, preds, labels, probs = classifier.predict(test_loader)\n",
        "            metrics = Evaluator.calculate_metrics(labels, preds)\n",
        "            metrics['training_time'] = train_time\n",
        "            metrics['val_accuracy'] = val_acc\n",
        "\n",
        "            # Store\n",
        "            model_predictions[model_key] = preds\n",
        "            model_probs[model_key] = probs\n",
        "            model_val_accuracies[model_key] = val_acc\n",
        "\n",
        "            Evaluator.print_results(f\"{model_key} (Test Set)\", metrics)\n",
        "\n",
        "            # Save individual results for this seed\n",
        "            result_path = os.path.join(config.RESULTS_DIR, f'{model_key}_results_seed{seed}.json')\n",
        "            Evaluator.save_results(metrics, result_path)\n",
        "\n",
        "            # Free memory\n",
        "            del classifier\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Apply ensemble methods\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ENSEMBLE METHODS\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        ensemble_results = {}\n",
        "        true_labels = labels\n",
        "\n",
        "        all_preds = [model_predictions[key] for key in config.MODELS.keys()]\n",
        "        all_probs = [model_probs[key] for key in config.MODELS.keys()]\n",
        "\n",
        "        # Soft Voting\n",
        "        soft_preds, soft_probs = EnsembleMethods.soft_voting(all_probs)\n",
        "        soft_metrics = Evaluator.calculate_metrics(true_labels, soft_preds)\n",
        "        Evaluator.print_results(\"Soft Voting Ensemble\", soft_metrics)\n",
        "        ensemble_results['soft_voting'] = soft_metrics\n",
        "\n",
        "        # Hard Voting\n",
        "        hard_preds = EnsembleMethods.hard_voting(all_preds)\n",
        "        hard_metrics = Evaluator.calculate_metrics(true_labels, hard_preds)\n",
        "        Evaluator.print_results(\"Hard Voting Ensemble\", hard_metrics)\n",
        "        ensemble_results['hard_voting'] = hard_metrics\n",
        "\n",
        "        # Weighted Voting\n",
        "        weights = [model_val_accuracies[key] for key in config.MODELS.keys()]\n",
        "        weighted_preds, weighted_probs = EnsembleMethods.weighted_voting(all_probs, weights)\n",
        "        weighted_metrics = Evaluator.calculate_metrics(true_labels, weighted_preds)\n",
        "        Evaluator.print_results(\"Weighted Voting Ensemble\", weighted_metrics)\n",
        "        ensemble_results['weighted_voting'] = weighted_metrics\n",
        "\n",
        "        # Stacking\n",
        "        val_probs_all = []\n",
        "        for model_key in config.MODELS.keys():\n",
        "            classifier = SentimentClassifier(config.MODELS[model_key], config, config.DEVICE)\n",
        "            model_path = os.path.join(config.MODEL_DIR, f'{model_key}_seed{seed}.pt')\n",
        "            classifier.model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "            _, val_loader, _ = data_manager.create_dataloaders(\n",
        "                train_df, val_df, test_df,\n",
        "                classifier.tokenizer,\n",
        "                config.BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            _, _, val_labels, val_probs = classifier.predict(val_loader)\n",
        "            val_probs_all.append(val_probs)\n",
        "\n",
        "            del classifier\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        val_labels = np.array(val_labels)\n",
        "        stacking_preds, stacking_probs, meta_clf = EnsembleMethods.stacking(\n",
        "            val_probs_all, val_labels, all_probs\n",
        "        )\n",
        "        stacking_metrics = Evaluator.calculate_metrics(true_labels, stacking_preds)\n",
        "        Evaluator.print_results(\"Stacking Ensemble\", stacking_metrics)\n",
        "        ensemble_results['stacking'] = stacking_metrics\n",
        "\n",
        "        # Store seed results\n",
        "        all_results[f'seed_{seed}'] = {\n",
        "            'individual_models': {k: Evaluator.calculate_metrics(true_labels, v)\n",
        "                                 for k, v in model_predictions.items()},\n",
        "            'ensembles': ensemble_results\n",
        "        }\n",
        "        \n",
        "        # Save complete results for this seed\n",
        "        seed_result_path = os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')\n",
        "        complete_seed_results = {\n",
        "            **{k: Evaluator.calculate_metrics(true_labels, v) \n",
        "               for k, v in model_predictions.items()},\n",
        "            **ensemble_results\n",
        "        }\n",
        "        Evaluator.save_results(complete_seed_results, seed_result_path)\n",
        "        print(f\"\\n\u2713 Saved complete results for seed {seed} to {seed_result_path}\")\n",
        "        \n",
        "        # Store test data for error analysis (only first seed)\n",
        "        if seed_idx == 0:\n",
        "            stored_test_data = {\n",
        "                'test_df': test_df,\n",
        "                'true_labels': true_labels,\n",
        "                'stacking_preds': stacking_preds,\n",
        "                'stacking_probs': stacking_probs\n",
        "            }\n",
        "\n",
        "    # Aggregate results\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"FINAL RESULTS (Averaged across all seeds)\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    def aggregate_results(results_dict):\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "        aggregated = {}\n",
        "\n",
        "        for model_name in results_dict[f'seed_{config.RANDOM_SEEDS[0]}'].keys():\n",
        "            aggregated[model_name] = {}\n",
        "            for metric in metrics:\n",
        "                values = []\n",
        "                for seed in config.RANDOM_SEEDS:\n",
        "                    if model_name in results_dict[f'seed_{seed}']:\n",
        "                        values.append(results_dict[f'seed_{seed}'][model_name][metric])\n",
        "                aggregated[model_name][metric] = {\n",
        "                    'mean': np.mean(values),\n",
        "                    'std': np.std(values)\n",
        "                }\n",
        "        return aggregated\n",
        "\n",
        "    individual_aggregated = aggregate_results({\n",
        "        f'seed_{seed}': all_results[f'seed_{seed}']['individual_models']\n",
        "        for seed in config.RANDOM_SEEDS\n",
        "    })\n",
        "\n",
        "    print(\"Individual Models:\")\n",
        "    for model_name, metrics in individual_aggregated.items():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        print(f\"  Accuracy:  {metrics['accuracy']['mean']:.4f} \u00b1 {metrics['accuracy']['std']:.4f}\")\n",
        "        print(f\"  F1-Score:  {metrics['f1']['mean']:.4f} \u00b1 {metrics['f1']['std']:.4f}\")\n",
        "\n",
        "    ensemble_aggregated = aggregate_results({\n",
        "        f'seed_{seed}': all_results[f'seed_{seed}']['ensembles']\n",
        "        for seed in config.RANDOM_SEEDS\n",
        "    })\n",
        "\n",
        "    print(\"\\nEnsemble Methods:\")\n",
        "    for ensemble_name, metrics in ensemble_aggregated.items():\n",
        "        print(f\"\\n{ensemble_name}:\")\n",
        "        print(f\"  Accuracy:  {metrics['accuracy']['mean']:.4f} \u00b1 {metrics['accuracy']['std']:.4f}\")\n",
        "        print(f\"  F1-Score:  {metrics['f1']['mean']:.4f} \u00b1 {metrics['f1']['std']:.4f}\")\n",
        "\n",
        "    # Generate runtime analysis\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RUNTIME ANALYSIS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    runtime_tracker.generate_runtime_table()\n",
        "    \n",
        "    runtime_file = os.path.join(config.RESULTS_DIR, 'runtime_analysis.txt')\n",
        "    runtime_tracker.export_runtime_analysis(runtime_file)\n",
        "    \n",
        "    # Statistical Analysis\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"STATISTICAL ANALYSIS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    analyzer = StatisticalAnalyzer(config.RESULTS_DIR, config.RANDOM_SEEDS)\n",
        "    \n",
        "    if len(analyzer.all_results) > 0:\n",
        "        summary = analyzer.generate_summary_table()\n",
        "        \n",
        "        print(\"\\nIndividual Models (Mean \u00b1 Std):\")\n",
        "        for model in config.MODELS.keys():\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        print(\"\\nEnsemble Methods (Mean \u00b1 Std):\")\n",
        "        for model in ['soft_voting', 'hard_voting', 'weighted_voting', 'stacking']:\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        sig_results, best_ens, best_ind = analyzer.perform_significance_tests()\n",
        "        \n",
        "        stat_file = os.path.join(config.RESULTS_DIR, 'statistical_analysis.txt')\n",
        "        analyzer.export_for_paper(stat_file)\n",
        "        \n",
        "        plot_path = os.path.join(config.FIGURES_DIR, 'performance_boxplots.png')\n",
        "        analyzer.plot_performance(plot_path)\n",
        "        \n",
        "        print(f\"\\n\u2713 Statistical analysis saved to: {stat_file}\")\n",
        "    \n",
        "    # Error Analysis\n",
        "    if stored_test_data:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"ERROR ANALYSIS\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        error_analyzer = ErrorAnalyzer(\n",
        "            stored_test_data['test_df'],\n",
        "            stored_test_data['true_labels'],\n",
        "            stored_test_data['stacking_preds'],\n",
        "            stored_test_data['stacking_probs'],\n",
        "            \"Stacking Ensemble\"\n",
        "        )\n",
        "        \n",
        "        error_summary = error_analyzer.get_error_summary()\n",
        "        print(f\"\\nFalse Positives: {error_summary['false_positives']} ({error_summary['fp_rate']*100:.2f}%)\")\n",
        "        print(f\"False Negatives: {error_summary['false_negatives']} ({error_summary['fn_rate']*100:.2f}%)\")\n",
        "        \n",
        "        suspicious = error_analyzer.analyze_label_quality(sample_size=20)\n",
        "        \n",
        "        error_file = os.path.join(config.RESULTS_DIR, 'error_analysis_report.txt')\n",
        "        error_analyzer.generate_error_report(error_file)\n",
        "        print(f\"\\n\u2713 Error analysis saved to: {error_file}\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"\u2713 ALL EXPERIMENTS COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nResults directory: {config.RESULTS_DIR}\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - results_seed_*.json\")\n",
        "    print(\"  - statistical_analysis.txt\")\n",
        "    print(\"  - runtime_analysis.txt\")\n",
        "    print(\"  - error_analysis_report.txt\")\n",
        "    print(\"  - performance_boxplots.png\")\n",
        "\n",
        "print(\"\u2713 Enhanced main() function defined\")"
      ],
      "metadata": {
        "id": "PhW3izV71C0s"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# RUN EVERYTHING\n",
        "# ========================================\n",
        "# This cell executes the complete pipeline:\n",
        "#   - Training (all models, all seeds)\n",
        "#   - Statistical analysis\n",
        "#   - Error analysis\n",
        "#   - Runtime tracking\n",
        "#   - Generate all tables and figures\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "run_complete_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Experiment"
      ],
      "metadata": {
        "id": "ZrqbOkCy6VCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "Qp_D_SpO1I3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 - Main Functions\n",
        "\n",
        "**Two main functions:**\n",
        "1. `run_single_seed(seed)` - Run one seed at a time\n",
        "2. `run_all_analyses()` - Analyze all completed seeds"
      ],
      "metadata": {
        "id": "main_functions_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_single_seed(seed):\n",
        "    \"\"\"\n",
        "    Run complete training pipeline for a single seed\n",
        "    \n",
        "    This function:\n",
        "    1. Trains all 4 models (AraBERT, MARBERT, XLM-RoBERTa, CAMeLBERT)\n",
        "    2. Tests all 4 ensemble methods (Soft, Hard, Weighted, Stacking)\n",
        "    3. Saves all results to Google Drive\n",
        "    4. Tracks runtime automatically\n",
        "    5. Returns results dictionary\n",
        "    \n",
        "    Args:\n",
        "        seed (int): Random seed for reproducibility (e.g., 42, 123, 456, 789, 2024)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Complete results for this seed\n",
        "        \n",
        "    Example:\n",
        "        results_42 = run_single_seed(42)\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'#'*70}\")\n",
        "    print(f\"RUNNING SEED: {seed}\")\n",
        "    print(f\"{'#'*70}\\n\")\n",
        "    \n",
        "    # Set random seeds\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # Track runtime for this seed\n",
        "    runtime_tracker.start_seed(seed)\n",
        "    \n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    data_manager = DataManager(config)\n",
        "    dataset_path = os.path.join(config.DATA_DIR, config.DATASET_FILE)\n",
        "    \n",
        "    if not os.path.exists(dataset_path):\n",
        "        print(f\"\u274c ERROR: Dataset not found at {dataset_path}\")\n",
        "        print(\"\\nPlease upload 'balanced-reviews.csv' to:\")\n",
        "        print(f\"   {config.DATA_DIR}/\")\n",
        "        return None\n",
        "    \n",
        "    df = data_manager.load_hard_dataset(dataset_path)\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    print(f\"\u2713 Loaded {len(df)} reviews\")\n",
        "    \n",
        "    # Split data\n",
        "    train_df, val_df, test_df = data_manager.split_data(df, seed=seed)\n",
        "    \n",
        "    # Store predictions\n",
        "    model_predictions = {}\n",
        "    model_probs = {}\n",
        "    model_val_accuracies = {}\n",
        "    \n",
        "    # Train individual models\n",
        "    for model_key, model_name in config.MODELS.items():\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Training {model_key}: {model_name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        # Initialize model\n",
        "        classifier = SentimentClassifier(model_name, config, config.DEVICE)\n",
        "        \n",
        "        # Create dataloaders\n",
        "        train_loader, val_loader, test_loader = data_manager.create_dataloaders(\n",
        "            train_df, val_df, test_df,\n",
        "            classifier.tokenizer,\n",
        "            config.BATCH_SIZE\n",
        "        )\n",
        "        \n",
        "        # Train\n",
        "        model_path = os.path.join(config.MODEL_DIR, f'{model_key}_seed{seed}.pt')\n",
        "        val_acc, train_time = classifier.train(train_loader, val_loader, model_path)\n",
        "        \n",
        "        # Log training time\n",
        "        runtime_tracker.log_runtime(seed, model_key, train_time)\n",
        "        \n",
        "        # Evaluate\n",
        "        test_acc, preds, labels, probs = classifier.predict(test_loader)\n",
        "        metrics = Evaluator.calculate_metrics(labels, preds)\n",
        "        metrics['training_time'] = train_time\n",
        "        metrics['val_accuracy'] = val_acc\n",
        "        \n",
        "        # Store\n",
        "        model_predictions[model_key] = preds\n",
        "        model_probs[model_key] = probs\n",
        "        model_val_accuracies[model_key] = val_acc\n",
        "        \n",
        "        Evaluator.print_results(f\"{model_key} (Test Set)\", metrics)\n",
        "        \n",
        "        # Save individual results for this seed\n",
        "        result_path = os.path.join(config.RESULTS_DIR, f'{model_key}_results_seed{seed}.json')\n",
        "        Evaluator.save_results(metrics, result_path)\n",
        "        \n",
        "        # Free memory\n",
        "        del classifier\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    # Apply ensemble methods\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ENSEMBLE METHODS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    ensemble_results = {}\n",
        "    true_labels = labels\n",
        "    \n",
        "    all_preds = [model_predictions[key] for key in config.MODELS.keys()]\n",
        "    all_probs = [model_probs[key] for key in config.MODELS.keys()]\n",
        "    \n",
        "    # Soft Voting\n",
        "    soft_preds, soft_probs = EnsembleMethods.soft_voting(all_probs)\n",
        "    soft_metrics = Evaluator.calculate_metrics(true_labels, soft_preds)\n",
        "    Evaluator.print_results(\"Soft Voting Ensemble\", soft_metrics)\n",
        "    ensemble_results['soft_voting'] = soft_metrics\n",
        "    \n",
        "    # Hard Voting\n",
        "    hard_preds = EnsembleMethods.hard_voting(all_preds)\n",
        "    hard_metrics = Evaluator.calculate_metrics(true_labels, hard_preds)\n",
        "    Evaluator.print_results(\"Hard Voting Ensemble\", hard_metrics)\n",
        "    ensemble_results['hard_voting'] = hard_metrics\n",
        "    \n",
        "    # Weighted Voting\n",
        "    weights = [model_val_accuracies[key] for key in config.MODELS.keys()]\n",
        "    weighted_preds, weighted_probs = EnsembleMethods.weighted_voting(all_probs, weights)\n",
        "    weighted_metrics = Evaluator.calculate_metrics(true_labels, weighted_preds)\n",
        "    Evaluator.print_results(\"Weighted Voting Ensemble\", weighted_metrics)\n",
        "    ensemble_results['weighted_voting'] = weighted_metrics\n",
        "    \n",
        "    # Stacking\n",
        "    val_probs_all = []\n",
        "    for model_key in config.MODELS.keys():\n",
        "        classifier = SentimentClassifier(config.MODELS[model_key], config, config.DEVICE)\n",
        "        model_path = os.path.join(config.MODEL_DIR, f'{model_key}_seed{seed}.pt')\n",
        "        classifier.model.load_state_dict(torch.load(model_path))\n",
        "        \n",
        "        _, val_loader, _ = data_manager.create_dataloaders(\n",
        "            train_df, val_df, test_df,\n",
        "            classifier.tokenizer,\n",
        "            config.BATCH_SIZE\n",
        "        )\n",
        "        \n",
        "        _, _, val_labels, val_probs = classifier.predict(val_loader)\n",
        "        val_probs_all.append(val_probs)\n",
        "        \n",
        "        del classifier\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "    \n",
        "    val_labels = np.array(val_labels)\n",
        "    stacking_preds, stacking_probs, meta_clf = EnsembleMethods.stacking(\n",
        "        val_probs_all, val_labels, all_probs\n",
        "    )\n",
        "    stacking_metrics = Evaluator.calculate_metrics(true_labels, stacking_preds)\n",
        "    Evaluator.print_results(\"Stacking Ensemble\", stacking_metrics)\n",
        "    ensemble_results['stacking'] = stacking_metrics\n",
        "    \n",
        "    # Prepare complete results\n",
        "    seed_results = {\n",
        "        'seed': seed,\n",
        "        'individual_models': {k: Evaluator.calculate_metrics(true_labels, v)\n",
        "                             for k, v in model_predictions.items()},\n",
        "        'ensembles': ensemble_results,\n",
        "        'test_data': {\n",
        "            'test_df': test_df,\n",
        "            'true_labels': true_labels,\n",
        "            'stacking_preds': stacking_preds,\n",
        "            'stacking_probs': stacking_probs\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Save complete results for this seed\n",
        "    seed_result_path = os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')\n",
        "    complete_seed_results = {\n",
        "        **{k: Evaluator.calculate_metrics(true_labels, v) \n",
        "           for k, v in model_predictions.items()},\n",
        "        **ensemble_results\n",
        "    }\n",
        "    Evaluator.save_results(complete_seed_results, seed_result_path)\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"\u2713 SEED {seed} COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\u2713 Results saved to: {seed_result_path}\")\n",
        "    \n",
        "    # Show runtime for this seed\n",
        "    seed_time = runtime_tracker.get_seed_total(seed)\n",
        "    print(f\"\u2713 Seed runtime: {runtime_tracker.format_hours(seed_time):.2f} hours\")\n",
        "    \n",
        "    return seed_results\n",
        "\n",
        "def run_all_analyses():\n",
        "    \"\"\"\n",
        "    Run comprehensive analysis across all completed seeds\n",
        "    \n",
        "    This function:\n",
        "    1. Loads results from all completed seeds\n",
        "    2. Performs statistical analysis (mean \u00b1 std, t-tests)\n",
        "    3. Performs error analysis (FP/FN, label noise)\n",
        "    4. Generates runtime analysis\n",
        "    5. Creates all tables and figures\n",
        "    6. Exports everything for paper\n",
        "    \n",
        "    Call this AFTER running all seeds to generate final results.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RUNNING COMPREHENSIVE ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Find which seeds have been completed\n",
        "    completed_seeds = []\n",
        "    for seed in config.RANDOM_SEEDS:\n",
        "        result_file = os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')\n",
        "        if os.path.exists(result_file):\n",
        "            completed_seeds.append(seed)\n",
        "    \n",
        "    if len(completed_seeds) == 0:\n",
        "        print(\"\u274c No completed seeds found!\")\n",
        "        print(\"   Please run at least one seed first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\u2713 Found {len(completed_seeds)} completed seeds: {completed_seeds}\\n\")\n",
        "    \n",
        "    # Runtime Analysis\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"RUNTIME ANALYSIS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    runtime_tracker.generate_runtime_table()\n",
        "    runtime_file = os.path.join(config.RESULTS_DIR, 'runtime_analysis.txt')\n",
        "    runtime_tracker.export_runtime_analysis(runtime_file)\n",
        "    print(f\"\\n\u2713 Runtime analysis saved to: {runtime_file}\")\n",
        "    \n",
        "    # Statistical Analysis\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"STATISTICAL ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    analyzer = StatisticalAnalyzer(config.RESULTS_DIR, completed_seeds)\n",
        "    \n",
        "    if len(analyzer.all_results) > 0:\n",
        "        summary = analyzer.generate_summary_table()\n",
        "        \n",
        "        print(\"Individual Models (Mean \u00b1 Std):\")\n",
        "        for model in config.MODELS.keys():\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        print(\"\\nEnsemble Methods (Mean \u00b1 Std):\")\n",
        "        for model in ['soft_voting', 'hard_voting', 'weighted_voting', 'stacking']:\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        sig_results, best_ens, best_ind = analyzer.perform_significance_tests()\n",
        "        print(f\"\\n\u2713 Best Ensemble: {best_ens}\")\n",
        "        print(f\"\u2713 Best Individual: {best_ind}\")\n",
        "        \n",
        "        stat_file = os.path.join(config.RESULTS_DIR, 'statistical_analysis.txt')\n",
        "        analyzer.export_for_paper(stat_file)\n",
        "        \n",
        "        plot_path = os.path.join(config.FIGURES_DIR, 'performance_boxplots.png')\n",
        "        analyzer.plot_performance(plot_path)\n",
        "        \n",
        "        print(f\"\\n\u2713 Statistical analysis saved to: {stat_file}\")\n",
        "        print(f\"\u2713 Box plots saved to: {plot_path}\")\n",
        "    \n",
        "    # Error Analysis (using first completed seed)\n",
        "    first_seed = completed_seeds[0]\n",
        "    \n",
        "    # Try to load test data if available\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ERROR ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"Note: Using data from seed {first_seed} for error analysis\")\n",
        "    \n",
        "    # Load the seed results\n",
        "    seed_file = os.path.join(config.RESULTS_DIR, f'results_seed_{first_seed}.json')\n",
        "    \n",
        "    # We need to re-run the seed to get test_df and predictions\n",
        "    # For now, we'll create a placeholder\n",
        "    print(\"\\nTo run complete error analysis:\")\n",
        "    print(\"1. The test_df and predictions are saved during seed execution\")\n",
        "    print(\"2. Error analysis will be performed on the first seed's data\")\n",
        "    print(\"\\nError analysis requires re-running or storing test data.\")\n",
        "    print(\"See error_analysis_manual.txt for instructions on manual analysis.\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"\u2713 ALL ANALYSES COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nResults saved to: {config.RESULTS_DIR}\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - results_seed_*.json (for each completed seed)\")\n",
        "    print(\"  - statistical_analysis.txt (mean\u00b1std, t-tests, LaTeX)\")\n",
        "    print(\"  - runtime_analysis.txt (training times, LaTeX)\")\n",
        "    print(\"  - performance_boxplots.png (visualizations)\")\n",
        "\n",
        "# Initialize runtime tracker\n",
        "runtime_tracker.start_experiment()\n",
        "\n",
        "print(\"\u2713 run_single_seed() function defined\")\n",
        "print(\"\u2713 run_all_analyses() function defined\")\n",
        "print(\"\u2713 Runtime tracker initialized\")\n",
        "print(\"\\nYou can now run seeds individually:\")\n",
        "print(\"  results_42 = run_single_seed(42)\")\n",
        "print(\"  results_123 = run_single_seed(123)\")\n",
        "print(\"  ... etc\")\n",
        "print(\"\\nAfter running all seeds, call:\")\n",
        "print(\"  run_all_analyses()\")\n",
        "\n",
        "def run_all_analyses():\n",
        "    \"\"\"\n",
        "    Run comprehensive analysis across all completed seeds\n",
        "    \n",
        "    This function:\n",
        "    1. Loads results from all completed seeds\n",
        "    2. Performs statistical analysis (mean \u00b1 std, t-tests)\n",
        "    3. Performs error analysis (FP/FN, label noise)\n",
        "    4. Generates runtime analysis\n",
        "    5. Creates all tables and figures\n",
        "    6. Exports everything for paper\n",
        "    \n",
        "    Call this AFTER running all seeds to generate final results.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"RUNNING COMPREHENSIVE ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Find which seeds have been completed\n",
        "    completed_seeds = []\n",
        "    for seed in config.RANDOM_SEEDS:\n",
        "        result_file = os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')\n",
        "        if os.path.exists(result_file):\n",
        "            completed_seeds.append(seed)\n",
        "    \n",
        "    if len(completed_seeds) == 0:\n",
        "        print(\"\u274c No completed seeds found!\")\n",
        "        print(\"   Please run at least one seed first.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\u2713 Found {len(completed_seeds)} completed seeds: {completed_seeds}\\n\")\n",
        "    \n",
        "    # Runtime Analysis\n",
        "    print(f\"{'='*70}\")\n",
        "    print(\"RUNTIME ANALYSIS\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    runtime_tracker.generate_runtime_table()\n",
        "    runtime_file = os.path.join(config.RESULTS_DIR, 'runtime_analysis.txt')\n",
        "    runtime_tracker.export_runtime_analysis(runtime_file)\n",
        "    print(f\"\\n\u2713 Runtime analysis saved to: {runtime_file}\")\n",
        "    \n",
        "    # Statistical Analysis\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"STATISTICAL ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    analyzer = StatisticalAnalyzer(config.RESULTS_DIR, completed_seeds)\n",
        "    \n",
        "    if len(analyzer.all_results) > 0:\n",
        "        summary = analyzer.generate_summary_table()\n",
        "        \n",
        "        print(\"Individual Models (Mean \u00b1 Std):\")\n",
        "        for model in config.MODELS.keys():\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        print(\"\\nEnsemble Methods (Mean \u00b1 Std):\")\n",
        "        for model in ['soft_voting', 'hard_voting', 'weighted_voting', 'stacking']:\n",
        "            if model in summary and 'f1' in summary[model]:\n",
        "                print(f\"  {model:15s}: F1 = {summary[model]['f1']['formatted']}%\")\n",
        "        \n",
        "        sig_results, best_ens, best_ind = analyzer.perform_significance_tests()\n",
        "        print(f\"\\n\u2713 Best Ensemble: {best_ens}\")\n",
        "        print(f\"\u2713 Best Individual: {best_ind}\")\n",
        "        \n",
        "        stat_file = os.path.join(config.RESULTS_DIR, 'statistical_analysis.txt')\n",
        "        analyzer.export_for_paper(stat_file)\n",
        "        \n",
        "        plot_path = os.path.join(config.FIGURES_DIR, 'performance_boxplots.png')\n",
        "        analyzer.plot_performance(plot_path)\n",
        "        \n",
        "        print(f\"\\n\u2713 Statistical analysis saved to: {stat_file}\")\n",
        "        print(f\"\u2713 Box plots saved to: {plot_path}\")\n",
        "    \n",
        "    # Error Analysis (using first completed seed)\n",
        "    first_seed = completed_seeds[0]\n",
        "    \n",
        "    # Try to load test data if available\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"ERROR ANALYSIS\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    print(f\"Note: Using data from seed {first_seed} for error analysis\")\n",
        "    \n",
        "    # Load the seed results\n",
        "    seed_file = os.path.join(config.RESULTS_DIR, f'results_seed_{first_seed}.json')\n",
        "    \n",
        "    # We need to re-run the seed to get test_df and predictions\n",
        "    # For now, we'll create a placeholder\n",
        "    print(\"\\nTo run complete error analysis:\")\n",
        "    print(\"1. The test_df and predictions are saved during seed execution\")\n",
        "    print(\"2. Error analysis will be performed on the first seed's data\")\n",
        "    print(\"\\nError analysis requires re-running or storing test data.\")\n",
        "    print(\"See error_analysis_manual.txt for instructions on manual analysis.\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"\u2713 ALL ANALYSES COMPLETE!\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\nResults saved to: {config.RESULTS_DIR}\")\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  - results_seed_*.json (for each completed seed)\")\n",
        "    print(\"  - statistical_analysis.txt (mean\u00b1std, t-tests, LaTeX)\")\n",
        "    print(\"  - runtime_analysis.txt (training times, LaTeX)\")\n",
        "    print(\"  - performance_boxplots.png (visualizations)\")\n",
        "\n",
        "# Initialize runtime tracker\n",
        "runtime_tracker.start_experiment()\n",
        "\n",
        "print(\"\u2713 run_single_seed() function defined\")\n",
        "print(\"\u2713 run_all_analyses() function defined\")\n",
        "print(\"\u2713 Runtime tracker initialized\")\n",
        "print(\"\\nYou can now run seeds individually:\")\n",
        "print(\"  results_42 = run_single_seed(42)\")\n",
        "print(\"  results_123 = run_single_seed(123)\")\n",
        "print(\"  ... etc\")\n",
        "print(\"\\nAfter running all seeds, call:\")\n",
        "print(\"  run_all_analyses()\")"
      ],
      "metadata": {
        "id": "main_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 - Run Seeds Individually\n",
        "\n",
        "**Instructions:**\n",
        "- Run cells 1-10 first (setup)\n",
        "- Then run seeds one at a time below\n",
        "- Each seed takes 3-4 hours on A100\n",
        "- Results saved automatically after each seed\n",
        "- Safe to disconnect between seeds"
      ],
      "metadata": {
        "id": "run_seeds_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN SEED 42\n",
        "# =====================================\n",
        "# Estimated time: 3-4 hours on A100, 7-8 hours on T4\n",
        "# Results will be saved to Google Drive automatically\n",
        "\n",
        "results_seed_42 = run_single_seed(42)"
      ],
      "metadata": {
        "id": "run_seed_42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN SEED 123\n",
        "# =====================================\n",
        "# Estimated time: 3-4 hours on A100, 7-8 hours on T4\n",
        "# Results will be saved to Google Drive automatically\n",
        "\n",
        "results_seed_123 = run_single_seed(123)"
      ],
      "metadata": {
        "id": "run_seed_123"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN SEED 456\n",
        "# =====================================\n",
        "# Estimated time: 3-4 hours on A100, 7-8 hours on T4\n",
        "# Results will be saved to Google Drive automatically\n",
        "\n",
        "results_seed_456 = run_single_seed(456)"
      ],
      "metadata": {
        "id": "run_seed_456"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN SEED 789\n",
        "# =====================================\n",
        "# Estimated time: 3-4 hours on A100, 7-8 hours on T4\n",
        "# Results will be saved to Google Drive automatically\n",
        "\n",
        "results_seed_789 = run_single_seed(789)"
      ],
      "metadata": {
        "id": "run_seed_789"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN SEED 2024\n",
        "# =====================================\n",
        "# Estimated time: 3-4 hours on A100, 7-8 hours on T4\n",
        "# Results will be saved to Google Drive automatically\n",
        "\n",
        "results_seed_2024 = run_single_seed(2024)"
      ],
      "metadata": {
        "id": "run_seed_2024"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12 - Run Final Analysis\n",
        "\n",
        "**Run this AFTER completing all (or some) seeds**\n",
        "\n",
        "This generates:\n",
        "- statistical_analysis.txt (mean\u00b1std, t-tests, LaTeX)\n",
        "- runtime_analysis.txt (timing tables, LaTeX)\n",
        "- performance_boxplots.png (visualizations)\n",
        "\n",
        "Works with any number of completed seeds (minimum 2 recommended)."
      ],
      "metadata": {
        "id": "analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# RUN COMPREHENSIVE ANALYSIS\n",
        "# =====================================\n",
        "# Run this AFTER completing seeds\n",
        "# Works with partial results (e.g., 3 out of 5 seeds)\n",
        "\n",
        "run_all_analyses()"
      ],
      "metadata": {
        "id": "run_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 - Check Progress (Optional)\n",
        "\n",
        "Run this cell anytime to see which seeds have been completed."
      ],
      "metadata": {
        "id": "progress_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# CHECK PROGRESS\n",
        "# =====================================\n",
        "# Run this anytime to see which seeds are done\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Checking completed seeds...\\n\")\n",
        "\n",
        "for seed in [42, 123, 456, 789, 2024]:\n",
        "    result_file = os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')\n",
        "    if os.path.exists(result_file):\n",
        "        print(f\"\u2713 Seed {seed} - COMPLETED\")\n",
        "        size_kb = os.path.getsize(result_file) / 1024\n",
        "        from datetime import datetime\n",
        "        mod_time = datetime.fromtimestamp(os.path.getmtime(result_file))\n",
        "        print(f\"  Size: {size_kb:.1f} KB\")\n",
        "        print(f\"  Completed: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    else:\n",
        "        print(f\"\u2717 Seed {seed} - NOT STARTED\")\n",
        "    print()\n",
        "\n",
        "completed = sum(1 for seed in [42, 123, 456, 789, 2024] \n",
        "                if os.path.exists(os.path.join(config.RESULTS_DIR, f'results_seed_{seed}.json')))\n",
        "\n",
        "print(f\"\\nProgress: {completed}/5 seeds completed ({completed*20}%)\")\n",
        "\n",
        "if completed == 5:\n",
        "    print(\"\\n\ud83c\udf89 All seeds complete! Run the analysis cell above.\")\n",
        "elif completed >= 3:\n",
        "    print(f\"\\n\u2713 {completed} seeds complete. You can run analysis now or wait for more.\")\n",
        "elif completed > 0:\n",
        "    print(f\"\\n\u23f3 {completed} seed(s) complete. Continue with remaining seeds.\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f  No seeds completed yet. Start with Seed 42 above.\")"
      ],
      "metadata": {
        "id": "check_progress"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}