{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Sentiment Analysis System: Lexicon + Stacking Ensemble\n",
        "\n",
        "**Goal:** Improve from 96.40% (stacking baseline) to 96.9-97.4% by combining with lexicon-based analyzer\n",
        "\n",
        "**What This Notebook Does:**\n",
        "1. Loads pre-trained stacking ensemble model\n",
        "2. Implements Arabic lexicon-based analyzer\n",
        "3. Evaluates 3 fusion strategies\n",
        "4. Analyzes error reduction on 227 real errors\n",
        "5. Generates results for paper\n",
        "\n",
        "**HOW TO USE:**\n",
        "1. Runtime > Change runtime type > GPU (T4 is sufficient)\n",
        "2. Run cells in order\n",
        "3. Results saved to: `/content/drive/MyDrive/arabic_sentiment_analysis/hybrid_results/`\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed BERT experiments (saved models in Google Drive)\n",
        "- Test data available\n",
        "\n",
        "**Expected Runtime:** 1-2 hours"
      ],
      "metadata": {
        "id": "hybrid_title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation"
      ],
      "metadata": {
        "id": "setup_title"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"HYBRID SENTIMENT SYSTEM - GOOGLE COLAB SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Install required packages (lightweight - no new transformers needed)\n",
        "print(\"\\nðŸ“¦ Installing required packages...\")\n",
        "!pip install -q transformers torch scikit-learn pandas numpy tqdm matplotlib seaborn\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\nðŸ’¾ Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create new directories for hybrid experiment\n",
        "import os\n",
        "project_path = '/content/drive/MyDrive/arabic_sentiment_analysis'\n",
        "hybrid_path = f'{project_path}/hybrid_experiment'\n",
        "\n",
        "# Create hybrid experiment directories\n",
        "os.makedirs(f'{hybrid_path}/results', exist_ok=True)\n",
        "os.makedirs(f'{hybrid_path}/figures', exist_ok=True)\n",
        "os.makedirs(f'{hybrid_path}/analysis', exist_ok=True)\n",
        "\n",
        "print(f\"\\nâœ“ Hybrid experiment directory created at: {hybrid_path}\")\n",
        "\n",
        "# Verify existing resources\n",
        "print(\"\\nðŸ” Verifying prerequisites...\")\n",
        "\n",
        "# Check for saved models\n",
        "model_dir = f'{project_path}/saved_models'\n",
        "if os.path.exists(model_dir):\n",
        "    model_files = os.listdir(model_dir)\n",
        "    print(f\"  âœ“ Found {len(model_files)} saved model files\")\n",
        "else:\n",
        "    print(\"  âš ï¸  WARNING: No saved models found. Run BERT experiments first!\")\n",
        "\n",
        "# Check for data\n",
        "data_file = f'{project_path}/data/balanced-reviews.csv'\n",
        "if os.path.exists(data_file):\n",
        "    print(\"  âœ“ Dataset found\")\n",
        "else:\n",
        "    print(\"  âš ï¸  WARNING: Dataset not found. Upload balanced-reviews.csv first!\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nâœ“ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(\"   Note: GPU not strictly required for lexicon analysis, but helpful for loading models\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No GPU detected. Lexicon analysis will run on CPU (still fast!)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Setup complete! You can now run the next cells.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration"
      ],
      "metadata": {
        "id": "config_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridConfig:\n",
        "    \"\"\"Configuration for hybrid experiment\"\"\"\n",
        "    \n",
        "    # Paths (Google Drive) - matching your existing structure\n",
        "    PROJECT_PATH = '/content/drive/MyDrive/arabic_sentiment_analysis'\n",
        "    HYBRID_PATH = f'{PROJECT_PATH}/hybrid_experiment'\n",
        "    \n",
        "    # Original experiment paths\n",
        "    DATA_DIR = f'{PROJECT_PATH}/data'\n",
        "    MODEL_DIR = f'{PROJECT_PATH}/saved_models'\n",
        "    ORIGINAL_RESULTS = f'{PROJECT_PATH}/results'\n",
        "    \n",
        "    # New hybrid results paths\n",
        "    HYBRID_RESULTS = f'{HYBRID_PATH}/results'\n",
        "    HYBRID_FIGURES = f'{HYBRID_PATH}/figures'\n",
        "    HYBRID_ANALYSIS = f'{HYBRID_PATH}/analysis'\n",
        "    \n",
        "    # Dataset settings (same as original)\n",
        "    DATASET_FILE = 'balanced-reviews.csv'\n",
        "    TRAIN_RATIO = 0.8\n",
        "    TEST_RATIO = 0.2\n",
        "    \n",
        "    # Model configurations (same as original)\n",
        "    MODELS = {\n",
        "        'arabert': 'aubmindlab/bert-base-arabert',\n",
        "        'marbert': 'UBC-NLP/MARBERT',\n",
        "        'xlm-roberta': 'xlm-roberta-base',\n",
        "        'camelbert': 'CAMeL-Lab/bert-base-arabic-camelbert-msa'\n",
        "    }\n",
        "    \n",
        "    # Seeds (must match original experiment)\n",
        "    RANDOM_SEEDS = [42, 123, 456, 789, 2024]\n",
        "    \n",
        "    # Hybrid system settings\n",
        "    FUSION_STRATEGIES = ['override', 'weighted', 'disagreement']\n",
        "    \n",
        "    # Confidence thresholds for fusion strategies\n",
        "    TRANSFORMER_CONFIDENCE_LOW = 0.6   # Below this, consider lexicon\n",
        "    LEXICON_CONFIDENCE_HIGH = 0.75     # Above this, trust lexicon\n",
        "    LEXICON_MIN_TERMS = 3              # Minimum sentiment terms for override\n",
        "    \n",
        "    # Device\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    # Batch size for predictions\n",
        "    BATCH_SIZE = 32\n",
        "    MAX_LENGTH = 512\n",
        "\n",
        "config = HybridConfig()\n",
        "print(\"âœ“ Hybrid configuration loaded\")\n",
        "print(f\"  Device: {config.DEVICE}\")\n",
        "print(f\"  Hybrid results: {config.HYBRID_RESULTS}\")\n",
        "print(f\"  Fusion strategies: {config.FUSION_STRATEGIES}\")\n",
        "print(f\"  Seeds to evaluate: {config.RANDOM_SEEDS}\")"
      ],
      "metadata": {
        "id": "config_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Data and Pre-trained Models"
      ],
      "metadata": {
        "id": "load_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"LOADING DATA AND MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load dataset\n",
        "print(\"\\nðŸ“Š Loading dataset...\")\n",
        "data_path = f\"{config.DATA_DIR}/{config.DATASET_FILE}\"\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"  Total samples: {len(df):,}\")\n",
        "print(f\"  Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Prepare data (same split as original experiment)\n",
        "print(\"\\nðŸ”€ Creating train/test split...\")\n",
        "\n",
        "# Assuming your dataset has 'text' and 'label' columns\n",
        "# Adjust column names if different\n",
        "if 'review' in df.columns:\n",
        "    text_col = 'review'\n",
        "elif 'text' in df.columns:\n",
        "    text_col = 'text'\n",
        "else:\n",
        "    text_col = df.columns[0]  # Assume first column is text\n",
        "\n",
        "if 'rating' in df.columns:\n",
        "    # Convert ratings to binary (positive/negative)\n",
        "    df['label'] = (df['rating'] >= 3).astype(int)\n",
        "elif 'label' not in df.columns:\n",
        "    print(\"  âš ï¸  WARNING: No 'label' column found. Please check your data format.\")\n",
        "\n",
        "texts = df[text_col].values\n",
        "labels = df['label'].values\n",
        "\n",
        "# Split with same random state as original experiment\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size=config.TEST_RATIO,\n",
        "    random_state=42,  # Must match original!\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(f\"  Train size: {len(X_train):,}\")\n",
        "print(f\"  Test size: {len(X_test):,}\")\n",
        "print(f\"  Test positive: {y_test.sum():,} ({y_test.mean()*100:.1f}%)\")\n",
        "print(f\"  Test negative: {(1-y_test).sum():,} ({(1-y_test.mean())*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Data loading complete!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "load_data_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Arabic Lexicon-Based Analyzer Implementation"
      ],
      "metadata": {
        "id": "lexicon_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Dict, Tuple, List\n",
        "from collections import Counter\n",
        "\n",
        "class ArabicLexiconAnalyzer:\n",
        "    \"\"\"Lexicon-based sentiment analyzer for Arabic text\"\"\"\n",
        "    \n",
        "    def __init__(self, custom_lexicon: Dict[str, float] = None):\n",
        "        self.lexicon = custom_lexicon if custom_lexicon else self._load_default_lexicon()\n",
        "        \n",
        "        # Negation words\n",
        "        self.negations = [\n",
        "            \"Ù„Ø§\", \"Ù„Ù…\", \"Ù„Ù†\", \"Ù„ÙŠØ³\", \"Ù„ÙŠØ³Øª\", \"Ù„Ø³Ù†Ø§\",\n",
        "            \"ØºÙŠØ±\", \"Ø¨Ø¯ÙˆÙ†\", \"Ù…Ø§\", \"Ù…Ø´\", \"Ù…Ùˆ\"\n",
        "        ]\n",
        "        \n",
        "        # Intensifiers\n",
        "        self.intensifiers = {\n",
        "            \"Ø¬Ø¯Ø§\": 1.5, \"Ø¬Ø¯Ø§Ù‹\": 1.5,\n",
        "            \"ÙƒØ«ÙŠØ±\": 1.4, \"ÙƒØ«ÙŠØ±Ø§Ù‹\": 1.4, \"ÙƒØ«ÙŠØ±Ø§\": 1.4,\n",
        "            \"Ù„Ù„ØºØ§ÙŠØ©\": 1.6,\n",
        "            \"Ø­Ù‚Ø§\": 1.3, \"Ø­Ù‚Ø§Ù‹\": 1.3,\n",
        "            \"ÙØ¹Ù„Ø§\": 1.3, \"ÙØ¹Ù„Ø§Ù‹\": 1.3,\n",
        "            \"Ø£Ø¨Ø¯Ø§\": 1.5, \"Ø£Ø¨Ø¯Ø§Ù‹\": 1.5,\n",
        "            \"ØªÙ…Ø§Ù…Ø§\": 1.4, \"ØªÙ…Ø§Ù…Ø§Ù‹\": 1.4\n",
        "        }\n",
        "        \n",
        "        # Diminishers\n",
        "        self.diminishers = {\n",
        "            \"Ù‚Ù„ÙŠÙ„Ø§\": 0.5, \"Ù‚Ù„ÙŠÙ„Ø§Ù‹\": 0.5, \"Ù‚Ù„ÙŠÙ„\": 0.5,\n",
        "            \"Ø´ÙˆÙŠ\": 0.6, \"Ø´ÙˆÙŠØ©\": 0.6,\n",
        "            \"Ø¨Ø¹Ø¶\": 0.7, \"Ø¨Ø¹Ø¶Ø§Ù‹\": 0.7,\n",
        "            \"Ù†ÙˆØ¹Ø§\": 0.6, \"Ù†ÙˆØ¹Ø§Ù‹\": 0.6\n",
        "        }\n",
        "    \n",
        "    def _load_default_lexicon(self) -> Dict[str, float]:\n",
        "        \"\"\"Load comprehensive Arabic sentiment lexicon\"\"\"\n",
        "        lexicon = {\n",
        "            # POSITIVE TERMS (MSA)\n",
        "            \"Ø±Ø§Ø¦Ø¹\": 0.95, \"Ù…Ù…ØªØ§Ø²\": 0.9, \"Ù…Ø°Ù‡Ù„\": 0.95, \"Ø¹Ø¸ÙŠÙ…\": 0.9,\n",
        "            \"Ù…ØªÙ…ÙŠØ²\": 0.85, \"ÙØ§Ø®Ø±\": 0.9, \"Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ\": 0.9,\n",
        "            \"Ø¬Ù…ÙŠÙ„\": 0.8, \"Ø¬ÙŠØ¯\": 0.7, \"Ø­Ø³Ù†\": 0.7, \"Ø·ÙŠØ¨\": 0.7,\n",
        "            \"Ù†Ø¸ÙŠÙ\": 0.75, \"Ù†Ø¸ÙŠÙØ©\": 0.75, \"Ù…Ø±ØªØ¨\": 0.7,\n",
        "            \"Ù„Ø·ÙŠÙ\": 0.7, \"Ù…ØªØ¹Ø§ÙˆÙ†\": 0.75, \"Ù…Ø±ÙŠØ­\": 0.75,\n",
        "            \"ÙˆØ§Ø³Ø¹\": 0.6, \"ÙˆØ§Ø³Ø¹Ø©\": 0.6, \"ÙƒØ¨ÙŠØ±\": 0.6,\n",
        "            \n",
        "            # POSITIVE TERMS (DIALECTAL)\n",
        "            \"ÙƒÙˆÙŠØ³\": 0.7, \"ÙƒÙˆÙŠØ³Ø©\": 0.7,  # Egyptian\n",
        "            \"Ø²ÙŠÙ†\": 0.75, \"Ø²ÙŠÙ†Ø©\": 0.75,  # Gulf\n",
        "            \"Ù…Ù†ÙŠØ­\": 0.7, \"Ù…Ù†ÙŠØ­Ø©\": 0.7,  # Levantine\n",
        "            \"Ø­Ù„Ùˆ\": 0.75, \"Ø­Ù„ÙˆØ©\": 0.75,  # Universal\n",
        "            \"ØªÙ…Ø§Ù…\": 0.7,  # Egyptian\n",
        "            \"ÙŠÙ‡Ø¨Ù„\": 0.8,  # Gulf\n",
        "            \n",
        "            # NEGATIVE TERMS (MSA)\n",
        "            \"Ø³ÙŠØ¡\": -0.95, \"Ø³ÙŠØ¦Ø©\": -0.95, \"Ø±Ù‡ÙŠØ¨\": -0.9,\n",
        "            \"ÙØ¸ÙŠØ¹\": -0.95, \"ÙØ¸ÙŠØ¹Ø©\": -0.95, \"Ù…Ø±ÙˆØ¹\": -0.9,\n",
        "            \"ÙƒØ§Ø±Ø«Ø©\": -0.95, \"ÙƒØ§Ø±Ø«ÙŠ\": -0.95,\n",
        "            \"Ù‚Ø¯ÙŠÙ…\": -0.6, \"Ù‚Ø¯ÙŠÙ…Ø©\": -0.6,\n",
        "            \"Ù…ØªØ³Ø®\": -0.8, \"Ù…ØªØ³Ø®Ø©\": -0.8,\n",
        "            \"Ø¶Ø¹ÙŠÙ\": -0.7, \"Ø¶Ø¹ÙŠÙØ©\": -0.7,\n",
        "            \"Ù…Ø´ÙƒÙ„Ø©\": -0.7, \"Ù…Ø´Ø§ÙƒÙ„\": -0.75, \"Ù…Ø²Ø¹Ø¬\": -0.75,\n",
        "            \n",
        "            # NEGATIVE TERMS (DIALECTAL)\n",
        "            \"ÙˆØ­Ø´\": -0.8,  # Egyptian\n",
        "            \"Ø²ÙØª\": -0.9,  # Egyptian\n",
        "            \n",
        "            # DISAPPOINTMENT MARKERS\n",
        "            \"Ù„Ù„Ø£Ø³Ù\": -0.6, \"Ù…Ø¹\": -0.4,\n",
        "            \"Ù„ÙƒÙ†\": -0.3, \"Ø¨Ø³\": -0.3,\n",
        "            \"ØªÙˆÙ‚Ø¹Øª\": -0.4, \"ØªÙˆÙ‚Ø¹Ø§Øª\": -0.3,\n",
        "            \"Ù„ÙŠØ³\": -0.4, \"Ù„Ø§\": -0.3,\n",
        "            \n",
        "            # HOTEL-SPECIFIC TERMS\n",
        "            \"Ù†Ø¸Ø§ÙØ©\": 0.7, \"Ø®Ø¯Ù…Ø©\": 0.5, \"Ù…ÙˆÙ‚Ø¹\": 0.5,\n",
        "            \"Ù…ÙˆØ¸ÙÙˆÙ†\": 0.5, \"Ù…ÙˆØ¸ÙÙŠÙ†\": 0.5,\n",
        "            \"Ø¹Ø¯Ù…\": -0.5, \"Ø§Ù†Ø¹Ø¯Ø§Ù…\": -0.6,\n",
        "            \"Ø±Ø§Ø¦Ø­Ø©\": -0.4, \"Ø±ÙˆØ§Ø¦Ø­\": -0.5,\n",
        "            \"ØµØ±Ø§ØµÙŠØ±\": -0.9, \"Ø­Ø´Ø±Ø§Øª\": -0.8,\n",
        "            \"Ø¶Ø¬ÙŠØ¬\": -0.7, \"Ø¶ÙˆØ¶Ø§Ø¡\": -0.7, \"ØµÙˆØª\": -0.3,\n",
        "        }\n",
        "        return lexicon\n",
        "    \n",
        "    def detect_negation(self, tokens: List[str], index: int, window: int = 3) -> bool:\n",
        "        \"\"\"Detect if sentiment word is negated\"\"\"\n",
        "        start = max(0, index - window)\n",
        "        context = tokens[start:index]\n",
        "        return any(neg in context for neg in self.negations)\n",
        "    \n",
        "    def get_modifier(self, tokens: List[str], index: int) -> float:\n",
        "        \"\"\"Get intensifier/diminisher multiplier\"\"\"\n",
        "        # Check next word\n",
        "        if index + 1 < len(tokens):\n",
        "            next_word = tokens[index + 1]\n",
        "            if next_word in self.intensifiers:\n",
        "                return self.intensifiers[next_word]\n",
        "            if next_word in self.diminishers:\n",
        "                return self.diminishers[next_word]\n",
        "        \n",
        "        # Check previous word\n",
        "        if index > 0:\n",
        "            prev_word = tokens[index - 1]\n",
        "            if prev_word in self.intensifiers:\n",
        "                return self.intensifiers[prev_word]\n",
        "            if prev_word in self.diminishers:\n",
        "                return self.diminishers[prev_word]\n",
        "        \n",
        "        return 1.0\n",
        "    \n",
        "    def analyze(self, text: str) -> Tuple[float, float, Dict]:\n",
        "        \"\"\"Analyze sentiment using lexicon\"\"\"\n",
        "        tokens = text.split()\n",
        "        sentiment_scores = []\n",
        "        details = {\n",
        "            \"found_terms\": [],\n",
        "            \"negations\": 0,\n",
        "            \"intensifiers\": 0,\n",
        "            \"diminishers\": 0\n",
        "        }\n",
        "        \n",
        "        for i, token in enumerate(tokens):\n",
        "            if token in self.lexicon:\n",
        "                score = self.lexicon[token]\n",
        "                original_score = score\n",
        "                \n",
        "                # Apply negation\n",
        "                if self.detect_negation(tokens, i):\n",
        "                    score *= -1\n",
        "                    details[\"negations\"] += 1\n",
        "                \n",
        "                # Apply modifier\n",
        "                modifier = self.get_modifier(tokens, i)\n",
        "                if modifier > 1.0:\n",
        "                    details[\"intensifiers\"] += 1\n",
        "                elif modifier < 1.0:\n",
        "                    details[\"diminishers\"] += 1\n",
        "                score *= modifier\n",
        "                \n",
        "                sentiment_scores.append(score)\n",
        "                details[\"found_terms\"].append({\n",
        "                    \"term\": token,\n",
        "                    \"original_score\": original_score,\n",
        "                    \"final_score\": score\n",
        "                })\n",
        "        \n",
        "        if not sentiment_scores:\n",
        "            return 0.0, 0.0, details\n",
        "        \n",
        "        # Aggregate scores\n",
        "        avg_score = np.mean(sentiment_scores)\n",
        "        \n",
        "        # Confidence\n",
        "        num_terms = len(sentiment_scores)\n",
        "        term_confidence = min(1.0, num_terms / 5)\n",
        "        \n",
        "        # Agreement\n",
        "        if avg_score != 0:\n",
        "            same_sign = sum(1 for s in sentiment_scores if (s > 0) == (avg_score > 0))\n",
        "            agreement = same_sign / num_terms\n",
        "        else:\n",
        "            agreement = 0.5\n",
        "        \n",
        "        confidence = (term_confidence * 0.6 + agreement * 0.4)\n",
        "        \n",
        "        details[\"num_terms\"] = num_terms\n",
        "        details[\"agreement\"] = agreement\n",
        "        \n",
        "        return avg_score, confidence, details\n",
        "    \n",
        "    def predict(self, text: str) -> Tuple[int, float, Dict]:\n",
        "        \"\"\"Predict sentiment label\"\"\"\n",
        "        score, confidence, details = self.analyze(text)\n",
        "        label = 1 if score > 0 else 0\n",
        "        return label, confidence, details\n",
        "\n",
        "# Test lexicon analyzer\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING LEXICON ANALYZER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "lexicon = ArabicLexiconAnalyzer()\n",
        "\n",
        "# Test examples\n",
        "test_cases = [\n",
        "    \"Ø§Ù„ÙÙ†Ø¯Ù‚ Ø±Ø§Ø¦Ø¹ ÙˆØ§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ù…ØªØ§Ø²\",\n",
        "    \"Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø© Ø¬Ø¯Ø§Ù‹\",\n",
        "    \"Ø§Ù„ÙÙ†Ø¯Ù‚ ÙƒÙˆÙŠØ³ Ø¨Ø³ Ø§Ù„Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø©\",\n",
        "    \"Ù„ÙŠØ³ Ø³ÙŠØ¡\",\n",
        "    \"Ù„Ù„Ø£Ø³Ù Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ù„Ù… ØªÙƒÙ† Ø¬ÙŠØ¯Ø©\"\n",
        "]\n",
        "\n",
        "for text in test_cases:\n",
        "    label, conf, details = lexicon.predict(text)\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {conf:.2f})\")\n",
        "    print(f\"Terms found: {details['num_terms']}, Negations: {details['negations']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ“ Lexicon analyzer ready!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "lexicon_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Stacking Ensemble Model"
      ],
      "metadata": {
        "id": "load_model_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING STACKING ENSEMBLE MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Option 1: Load pre-trained stacking model if you saved it\n",
        "stacking_model_path = f\"{config.MODEL_DIR}/stacking_ensemble.pkl\"\n",
        "\n",
        "if os.path.exists(stacking_model_path):\n",
        "    print(\"\\nðŸ“¦ Loading pre-saved stacking model...\")\n",
        "    with open(stacking_model_path, 'rb') as f:\n",
        "        stacking_model = pickle.load(f)\n",
        "    print(\"  âœ“ Stacking model loaded\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Pre-saved stacking model not found.\")\n",
        "    print(\"\\nðŸ“¦ Loading individual models and building stacking...\")\n",
        "    print(\"   This will take a few minutes...\")\n",
        "    \n",
        "    # You'll need to implement this based on your original training code\n",
        "    # For now, we'll create a placeholder that shows the structure\n",
        "    \n",
        "    from sklearn.ensemble import StackingClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "    # Load individual models for each seed\n",
        "    # This is a simplified version - adapt based on your actual model structure\n",
        "    base_models = []\n",
        "    \n",
        "    for model_name in config.MODELS.keys():\n",
        "        model_path = f\"{config.MODEL_DIR}/{model_name}_seed42.pt\"  # Example\n",
        "        if os.path.exists(model_path):\n",
        "            # Load your trained model\n",
        "            # model = torch.load(model_path)\n",
        "            # base_models.append((model_name, model))\n",
        "            print(f\"  Found: {model_name}\")\n",
        "    \n",
        "    print(\"\\n  âš ï¸  Note: You'll need to adapt this cell to load your actual models\")\n",
        "    print(\"     See original training notebook for model loading code\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Model loading complete!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "load_model_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Hybrid System Implementation"
      ],
      "metadata": {
        "id": "hybrid_impl_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridSentimentAnalyzer:\n",
        "    \"\"\"Hybrid system combining Stacking Ensemble + Lexicon\"\"\"\n",
        "    \n",
        "    def __init__(self, stacking_model, lexicon_analyzer: ArabicLexiconAnalyzer):\n",
        "        self.stacking = stacking_model\n",
        "        self.lexicon = lexicon_analyzer\n",
        "        \n",
        "        # Statistics\n",
        "        self.stats = {\n",
        "            \"total_predictions\": 0,\n",
        "            \"lexicon_overrides\": 0,\n",
        "            \"agreements\": 0,\n",
        "            \"disagreements\": 0\n",
        "        }\n",
        "    \n",
        "    def predict(self, text: str, strategy: str = \"disagreement\", \n",
        "                return_details: bool = False) -> Tuple:\n",
        "        \"\"\"Make hybrid prediction\"\"\"\n",
        "        self.stats[\"total_predictions\"] += 1\n",
        "        \n",
        "        # Get transformer prediction\n",
        "        transformer_proba = self.stacking.predict_proba([text])[0]\n",
        "        transformer_label = np.argmax(transformer_proba)\n",
        "        transformer_conf = np.max(transformer_proba)\n",
        "        \n",
        "        # Get lexicon prediction\n",
        "        lexicon_label, lexicon_conf, lexicon_details = self.lexicon.predict(text)\n",
        "        \n",
        "        # Track agreement\n",
        "        if transformer_label == lexicon_label:\n",
        "            self.stats[\"agreements\"] += 1\n",
        "        else:\n",
        "            self.stats[\"disagreements\"] += 1\n",
        "        \n",
        "        # Decision info\n",
        "        info = {\n",
        "            \"text\": text[:100] + \"...\" if len(text) > 100 else text,\n",
        "            \"transformer_label\": int(transformer_label),\n",
        "            \"transformer_conf\": float(transformer_conf),\n",
        "            \"lexicon_label\": int(lexicon_label),\n",
        "            \"lexicon_conf\": float(lexicon_conf),\n",
        "            \"lexicon_details\": lexicon_details,\n",
        "            \"strategy\": strategy\n",
        "        }\n",
        "        \n",
        "        # Apply fusion strategy\n",
        "        if strategy == \"override\":\n",
        "            result = self._override_strategy(\n",
        "                transformer_label, transformer_conf,\n",
        "                lexicon_label, lexicon_conf, info\n",
        "            )\n",
        "        elif strategy == \"weighted\":\n",
        "            result = self._weighted_strategy(\n",
        "                transformer_label, transformer_conf,\n",
        "                lexicon_label, lexicon_conf, info\n",
        "            )\n",
        "        else:  # disagreement\n",
        "            result = self._disagreement_strategy(\n",
        "                transformer_label, transformer_conf,\n",
        "                lexicon_label, lexicon_conf, info\n",
        "            )\n",
        "        \n",
        "        if return_details:\n",
        "            return result\n",
        "        else:\n",
        "            return result[0], result[1]\n",
        "    \n",
        "    def _override_strategy(self, t_label, t_conf, l_label, l_conf, info):\n",
        "        \"\"\"Strategy 1: Confidence-based override\"\"\"\n",
        "        if t_conf < config.TRANSFORMER_CONFIDENCE_LOW and l_conf > config.LEXICON_CONFIDENCE_HIGH:\n",
        "            info[\"decision\"] = \"lexicon_override\"\n",
        "            info[\"reason\"] = f\"Low transformer conf ({t_conf:.2f}), high lexicon conf ({l_conf:.2f})\"\n",
        "            self.stats[\"lexicon_overrides\"] += 1\n",
        "            return l_label, l_conf, info\n",
        "        else:\n",
        "            info[\"decision\"] = \"transformer\"\n",
        "            info[\"reason\"] = f\"Transformer conf sufficient ({t_conf:.2f})\"\n",
        "            return t_label, t_conf, info\n",
        "    \n",
        "    def _weighted_strategy(self, t_label, t_conf, l_label, l_conf, info):\n",
        "        \"\"\"Strategy 2: Weighted combination\"\"\"\n",
        "        if t_conf > 0.8:\n",
        "            w_t, w_l = 0.9, 0.1\n",
        "        elif t_conf > 0.6:\n",
        "            w_t, w_l = 0.7, 0.3\n",
        "        else:\n",
        "            w_t, w_l = 0.5, 0.5\n",
        "        \n",
        "        t_score = 1 if t_label == 1 else -1\n",
        "        l_score = 1 if l_label == 1 else -1\n",
        "        \n",
        "        final_score = w_t * t_score + w_l * l_score\n",
        "        final_label = 1 if final_score > 0 else 0\n",
        "        final_conf = abs(final_score)\n",
        "        \n",
        "        info[\"decision\"] = \"weighted\"\n",
        "        info[\"weights\"] = {\"transformer\": w_t, \"lexicon\": w_l}\n",
        "        info[\"reason\"] = f\"Weighted: T={w_t:.1f}, L={w_l:.1f}\"\n",
        "        \n",
        "        return final_label, final_conf, info\n",
        "    \n",
        "    def _disagreement_strategy(self, t_label, t_conf, l_label, l_conf, info):\n",
        "        \"\"\"Strategy 3: Disagreement detection (RECOMMENDED)\"\"\"\n",
        "        if t_label != l_label:\n",
        "            # Disagreement detected\n",
        "            if (l_conf > config.LEXICON_CONFIDENCE_HIGH and \n",
        "                info[\"lexicon_details\"][\"num_terms\"] >= config.LEXICON_MIN_TERMS):\n",
        "                # Strong lexicon signal\n",
        "                info[\"decision\"] = \"lexicon_on_disagreement\"\n",
        "                info[\"reason\"] = f\"Strong lexicon ({l_conf:.2f}, {info['lexicon_details']['num_terms']} terms)\"\n",
        "                self.stats[\"lexicon_overrides\"] += 1\n",
        "                return l_label, l_conf, info\n",
        "            else:\n",
        "                # Weak lexicon - trust transformer\n",
        "                info[\"decision\"] = \"transformer_on_disagreement\"\n",
        "                info[\"reason\"] = f\"Weak lexicon ({l_conf:.2f}), trust transformer\"\n",
        "                return t_label, t_conf, info\n",
        "        else:\n",
        "            # Agreement\n",
        "            info[\"decision\"] = \"agreement\"\n",
        "            info[\"reason\"] = \"Transformer and lexicon agree\"\n",
        "            return t_label, max(t_conf, l_conf), info\n",
        "    \n",
        "    def evaluate(self, texts: List[str], true_labels: List[int], \n",
        "                 strategy: str = \"disagreement\") -> Dict:\n",
        "        \"\"\"Evaluate hybrid system on test set\"\"\"\n",
        "        predictions = []\n",
        "        confidences = []\n",
        "        decision_types = []\n",
        "        \n",
        "        from tqdm import tqdm\n",
        "        for text, true_label in tqdm(zip(texts, true_labels), total=len(texts), desc=f\"Evaluating {strategy}\"):\n",
        "            pred, conf, info = self.predict(text, strategy=strategy, return_details=True)\n",
        "            predictions.append(pred)\n",
        "            confidences.append(conf)\n",
        "            decision_types.append(info[\"decision\"])\n",
        "        \n",
        "        predictions = np.array(predictions)\n",
        "        true_labels = np.array(true_labels)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "        \n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions)\n",
        "        recall = recall_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions)\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        \n",
        "        decision_counts = Counter(decision_types)\n",
        "        \n",
        "        results = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1,\n",
        "            \"confusion_matrix\": cm.tolist(),\n",
        "            \"decision_breakdown\": dict(decision_counts),\n",
        "            \"avg_confidence\": np.mean(confidences),\n",
        "            \"stats\": self.stats.copy()\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ“ Hybrid System Implementation Ready!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "hybrid_impl_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluate Hybrid System"
      ],
      "metadata": {
        "id": "eval_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EVALUATING HYBRID SYSTEM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Evaluate baseline stacking ensemble\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BASELINE: STACKING ENSEMBLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "baseline_preds = stacking_model.predict(X_test)\n",
        "\n",
        "baseline_acc = accuracy_score(y_test, baseline_preds)\n",
        "baseline_prec = precision_score(y_test, baseline_preds)\n",
        "baseline_rec = recall_score(y_test, baseline_preds)\n",
        "baseline_f1 = f1_score(y_test, baseline_preds)\n",
        "\n",
        "print(f\"\\nAccuracy:  {baseline_acc:.4f} ({baseline_acc*100:.2f}%)\")\n",
        "print(f\"Precision: {baseline_prec:.4f} ({baseline_prec*100:.2f}%)\")\n",
        "print(f\"Recall:    {baseline_rec:.4f} ({baseline_rec*100:.2f}%)\")\n",
        "print(f\"F1-Score:  {baseline_f1:.4f} ({baseline_f1*100:.2f}%)\")\n",
        "\n",
        "baseline_results = {\n",
        "    'accuracy': baseline_acc,\n",
        "    'precision': baseline_prec,\n",
        "    'recall': baseline_rec,\n",
        "    'f1': baseline_f1\n",
        "}\n",
        "\n",
        "# 2. Evaluate hybrid strategies\n",
        "hybrid = HybridSentimentAnalyzer(stacking_model, lexicon)\n",
        "hybrid_results = {}\n",
        "\n",
        "for strategy in config.FUSION_STRATEGIES:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"HYBRID STRATEGY: {strategy.upper()}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Reset stats\n",
        "    hybrid.stats = {\n",
        "        \"total_predictions\": 0,\n",
        "        \"lexicon_overrides\": 0,\n",
        "        \"agreements\": 0,\n",
        "        \"disagreements\": 0\n",
        "    }\n",
        "    \n",
        "    # Evaluate\n",
        "    results = hybrid.evaluate(X_test, y_test, strategy=strategy)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nAccuracy:  {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\")\n",
        "    print(f\"Precision: {results['precision']:.4f} ({results['precision']*100:.2f}%)\")\n",
        "    print(f\"Recall:    {results['recall']:.4f} ({results['recall']*100:.2f}%)\")\n",
        "    print(f\"F1-Score:  {results['f1_score']:.4f} ({results['f1_score']*100:.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nÎ” F1 vs Baseline: {results['f1_score'] - baseline_f1:+.4f} ({(results['f1_score'] - baseline_f1)*100:+.2f}%)\")\n",
        "    \n",
        "    print(f\"\\nDecision Breakdown:\")\n",
        "    for decision_type, count in results['decision_breakdown'].items():\n",
        "        pct = count / len(X_test) * 100\n",
        "        print(f\"  {decision_type}: {count} ({pct:.1f}%)\")\n",
        "    \n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Agreements: {results['stats']['agreements']} ({results['stats']['agreements']/len(X_test)*100:.1f}%)\")\n",
        "    print(f\"  Disagreements: {results['stats']['disagreements']} ({results['stats']['disagreements']/len(X_test)*100:.1f}%)\")\n",
        "    print(f\"  Lexicon Overrides: {results['stats']['lexicon_overrides']} ({results['stats']['lexicon_overrides']/len(X_test)*100:.1f}%)\")\n",
        "    \n",
        "    hybrid_results[strategy] = results\n",
        "\n",
        "# 3. Comparison summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n{'Method':<30} {'Accuracy':<12} {'F1-Score':<12} {'Î” F1':<10}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "print(f\"{'Stacking (baseline)':<30} {baseline_acc:.4f}      {baseline_f1:.4f}       -\")\n",
        "\n",
        "for strategy, results in hybrid_results.items():\n",
        "    f1 = results['f1_score']\n",
        "    delta = f1 - baseline_f1\n",
        "    print(f\"{'+ ' + strategy.capitalize():<30} {results['accuracy']:.4f}      {f1:.4f}       {delta:+.4f}\")\n",
        "\n",
        "# Find best strategy\n",
        "best_strategy = max(hybrid_results.keys(), key=lambda k: hybrid_results[k]['f1_score'])\n",
        "best_f1 = hybrid_results[best_strategy]['f1_score']\n",
        "improvement = best_f1 - baseline_f1\n",
        "relative_improvement = (best_f1 - baseline_f1) / (1 - baseline_f1) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BEST HYBRID STRATEGY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nStrategy: {best_strategy.upper()}\")\n",
        "print(f\"F1-Score: {best_f1:.4f} ({best_f1*100:.2f}%)\")\n",
        "print(f\"Improvement: +{improvement:.4f} (+{improvement*100:.2f}%)\")\n",
        "print(f\"Relative Error Reduction: {relative_improvement:.1f}%\")\n",
        "\n",
        "# Save results\n",
        "all_results = {\n",
        "    'baseline': baseline_results,\n",
        "    'hybrid': hybrid_results,\n",
        "    'best_strategy': best_strategy,\n",
        "    'improvement': float(improvement),\n",
        "    'relative_improvement': float(relative_improvement)\n",
        "}\n",
        "\n",
        "results_path = f\"{config.HYBRID_RESULTS}/evaluation_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Results saved to: {results_path}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "eval_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Error Analysis"
      ],
      "metadata": {
        "id": "error_analysis_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DETAILED ERROR ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get baseline predictions\n",
        "baseline_preds = stacking_model.predict(X_test)\n",
        "baseline_errors = np.where(baseline_preds != y_test)[0]\n",
        "\n",
        "# Get hybrid predictions (best strategy)\n",
        "hybrid_preds = []\n",
        "for text in tqdm(X_test, desc=\"Getting hybrid predictions\"):\n",
        "    pred, _ = hybrid.predict(text, strategy=best_strategy)\n",
        "    hybrid_preds.append(pred)\n",
        "hybrid_preds = np.array(hybrid_preds)\n",
        "hybrid_errors = np.where(hybrid_preds != y_test)[0]\n",
        "\n",
        "# Analyze corrections and new errors\n",
        "corrected = set(baseline_errors) - set(hybrid_errors)\n",
        "new_errors = set(hybrid_errors) - set(baseline_errors)\n",
        "\n",
        "print(f\"\\nBaseline Errors: {len(baseline_errors):,}\")\n",
        "print(f\"Hybrid Errors: {len(hybrid_errors):,}\")\n",
        "print(f\"\\nErrors Corrected: {len(corrected):,} ({len(corrected)/len(baseline_errors)*100:.1f}%)\")\n",
        "print(f\"New Errors Introduced: {len(new_errors):,} ({len(new_errors)/len(baseline_errors)*100:.1f}%)\")\n",
        "print(f\"Net Improvement: {len(corrected) - len(new_errors):,}\")\n",
        "\n",
        "# Save error analysis\n",
        "error_analysis = {\n",
        "    'baseline_errors': int(len(baseline_errors)),\n",
        "    'hybrid_errors': int(len(hybrid_errors)),\n",
        "    'corrected': int(len(corrected)),\n",
        "    'new_errors': int(len(new_errors)),\n",
        "    'net_improvement': int(len(corrected) - len(new_errors))\n",
        "}\n",
        "\n",
        "# Show examples of corrections\n",
        "if len(corrected) > 0:\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"EXAMPLE CORRECTIONS (first 5)\")\n",
        "    print(\"-\"*70)\n",
        "    for i, idx in enumerate(list(corrected)[:5]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {X_test[idx][:100]}...\")\n",
        "        print(f\"True Label: {'Positive' if y_test[idx]==1 else 'Negative'}\")\n",
        "        print(f\"Baseline: {'Positive' if baseline_preds[idx]==1 else 'Negative'} (WRONG)\")\n",
        "        print(f\"Hybrid: {'Positive' if hybrid_preds[idx]==1 else 'Negative'} (CORRECT)\")\n",
        "        \n",
        "        # Show lexicon analysis\n",
        "        _, _, details = lexicon.predict(X_test[idx])\n",
        "        print(f\"Lexicon found: {details['num_terms']} terms, {details['negations']} negations\")\n",
        "\n",
        "# Show examples of new errors\n",
        "if len(new_errors) > 0:\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"NEW ERRORS INTRODUCED (first 3)\")\n",
        "    print(\"-\"*70)\n",
        "    for i, idx in enumerate(list(new_errors)[:3]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {X_test[idx][:100]}...\")\n",
        "        print(f\"True Label: {'Positive' if y_test[idx]==1 else 'Negative'}\")\n",
        "        print(f\"Baseline: {'Positive' if baseline_preds[idx]==1 else 'Negative'} (CORRECT)\")\n",
        "        print(f\"Hybrid: {'Positive' if hybrid_preds[idx]==1 else 'Negative'} (WRONG)\")\n",
        "\n",
        "# Save error analysis\n",
        "error_path = f\"{config.HYBRID_ANALYSIS}/error_analysis.json\"\n",
        "with open(error_path, 'w') as f:\n",
        "    json.dump(error_analysis, f, indent=2)\n",
        "\n",
        "print(f\"\\nâœ“ Error analysis saved to: {error_path}\")\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "id": "error_analysis_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Generate Figures for Paper"
      ],
      "metadata": {
        "id": "figures_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"GENERATING FIGURES FOR PAPER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Figure 1: Performance Comparison Bar Chart\n",
        "print(\"\\nðŸ“Š Creating Figure 1: Performance Comparison...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "methods = ['Stacking\\n(Baseline)', 'Confidence\\nOverride', 'Weighted\\nCombination', 'Disagreement\\nDetection']\n",
        "f1_scores = [\n",
        "    baseline_f1,\n",
        "    hybrid_results['override']['f1_score'],\n",
        "    hybrid_results['weighted']['f1_score'],\n",
        "    hybrid_results['disagreement']['f1_score']\n",
        "]\n",
        "\n",
        "colors = ['#4472C4', '#70AD47', '#70AD47', '#70AD47']\n",
        "bars = ax.bar(methods, [f*100 for f in f1_scores], color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "# Add value labels\n",
        "for bar, score in zip(bars, f1_scores):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "            f'{score*100:.2f}%',\n",
        "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('F1-Score (%)', fontsize=12, fontweight='bold')\n",
        "ax.set_xlabel('Method', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Hybrid System Performance Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_ylim([95.5, 97.5])\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add baseline line\n",
        "ax.axhline(y=baseline_f1*100, color='red', linestyle=':', linewidth=2, alpha=0.6, label='Baseline')\n",
        "ax.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig1_path = f\"{config.HYBRID_FIGURES}/figure1_performance_comparison.png\"\n",
        "plt.savefig(fig1_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"  âœ“ Saved to: {fig1_path}\")\n",
        "plt.close()\n",
        "\n",
        "# Figure 2: Error Breakdown\n",
        "print(\"\\nðŸ“Š Creating Figure 2: Error Breakdown...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "categories = ['Baseline\\nErrors', 'Errors\\nCorrected', 'New\\nErrors', 'Final\\nErrors']\n",
        "values = [\n",
        "    len(baseline_errors),\n",
        "    len(corrected),\n",
        "    len(new_errors),\n",
        "    len(hybrid_errors)\n",
        "]\n",
        "colors = ['#C55A11', '#70AD47', '#FFC000', '#C55A11']\n",
        "\n",
        "bars = ax.bar(categories, values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "# Add value labels\n",
        "for bar, value in zip(bars, values):\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height + 5,\n",
        "            f'{value}',\n",
        "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax.set_ylabel('Number of Errors', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Error Reduction Analysis', fontsize=14, fontweight='bold')\n",
        "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig2_path = f\"{config.HYBRID_FIGURES}/figure2_error_breakdown.png\"\n",
        "plt.savefig(fig2_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"  âœ“ Saved to: {fig2_path}\")\n",
        "plt.close()\n",
        "\n",
        "# Figure 3: Decision Breakdown (Disagreement Strategy)\n",
        "print(\"\\nðŸ“Š Creating Figure 3: Decision Breakdown...\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "decision_data = hybrid_results[best_strategy]['decision_breakdown']\n",
        "decisions = list(decision_data.keys())\n",
        "counts = list(decision_data.values())\n",
        "\n",
        "colors_pie = ['#70AD47', '#4472C4', '#FFC000', '#C55A11']\n",
        "explode = [0.05 if 'lexicon' in d else 0 for d in decisions]\n",
        "\n",
        "ax.pie(counts, labels=decisions, autopct='%1.1f%%', startangle=90, \n",
        "       colors=colors_pie, explode=explode, textprops={'fontsize': 11})\n",
        "ax.set_title(f'Decision Breakdown ({best_strategy.capitalize()} Strategy)', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "fig3_path = f\"{config.HYBRID_FIGURES}/figure3_decision_breakdown.png\"\n",
        "plt.savefig(fig3_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"  âœ“ Saved to: {fig3_path}\")\n",
        "plt.close()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âœ“ All figures generated successfully!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nFigures saved to: {config.HYBRID_FIGURES}\")\n",
        "print(\"\\nFigures created:\")\n",
        "print(\"  1. figure1_performance_comparison.png\")\n",
        "print(\"  2. figure2_error_breakdown.png\")\n",
        "print(\"  3. figure3_decision_breakdown.png\")"
      ],
      "metadata": {
        "id": "figures_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Generate Paper Results"
      ],
      "metadata": {
        "id": "paper_results_title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"GENERATING PAPER-READY RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create formatted tables for paper\n",
        "paper_results = f\"\"\"\n",
        "{'='*70}\n",
        "TABLE 6: HYBRID SYSTEM PERFORMANCE COMPARISON\n",
        "{'='*70}\n",
        "\n",
        "Method                          Accuracy    Precision   Recall     F1-Score    Î” F1\n",
        "{'â”€'*85}\n",
        "Stacking Ensemble (baseline)    {baseline_acc:.4f}      {baseline_prec:.4f}      {baseline_rec:.4f}     {baseline_f1:.4f}      -\n",
        "+ Confidence Override            {hybrid_results['override']['accuracy']:.4f}      {hybrid_results['override']['precision']:.4f}      {hybrid_results['override']['recall']:.4f}     {hybrid_results['override']['f1_score']:.4f}      {hybrid_results['override']['f1_score']-baseline_f1:+.4f}\n",
        "+ Weighted Combination           {hybrid_results['weighted']['accuracy']:.4f}      {hybrid_results['weighted']['precision']:.4f}      {hybrid_results['weighted']['recall']:.4f}     {hybrid_results['weighted']['f1_score']:.4f}      {hybrid_results['weighted']['f1_score']-baseline_f1:+.4f}\n",
        "+ Disagreement Detection         {hybrid_results['disagreement']['accuracy']:.4f}      {hybrid_results['disagreement']['precision']:.4f}      {hybrid_results['disagreement']['recall']:.4f}     {hybrid_results['disagreement']['f1_score']:.4f}      {hybrid_results['disagreement']['f1_score']-baseline_f1:+.4f} *\n",
        "\n",
        "Note: * indicates best hybrid strategy\n",
        "\n",
        "\n",
        "{'='*70}\n",
        "TABLE 7: ERROR REDUCTION ANALYSIS\n",
        "{'='*70}\n",
        "\n",
        "Error Type                      Count    Lexicon Corrected    Remaining\n",
        "{'â”€'*70}\n",
        "Total Errors (Baseline)         {len(baseline_errors):<8} -                    {len(hybrid_errors)}\n",
        "Errors Corrected by Hybrid      -        {len(corrected):<20} -\n",
        "New Errors Introduced           -        {len(new_errors):<20} -\n",
        "Net Error Reduction             -        {len(corrected) - len(new_errors):<20} -\n",
        "{'â”€'*70}\n",
        "\n",
        "Correction Rate: {len(corrected)/len(baseline_errors)*100:.1f}%\n",
        "Override Precision: {len(corrected)/(len(corrected)+len(new_errors))*100:.1f}%\n",
        "Net Improvement: {(len(corrected) - len(new_errors))/len(baseline_errors)*100:.1f}% of baseline errors\n",
        "\n",
        "\n",
        "{'='*70}\n",
        "KEY FINDINGS FOR PAPER\n",
        "{'='*70}\n",
        "\n",
        "1. PERFORMANCE IMPROVEMENT:\n",
        "   - Best Strategy: {best_strategy.upper()}\n",
        "   - F1-Score: {best_f1:.4f} ({best_f1*100:.2f}%)\n",
        "   - Absolute Improvement: +{improvement:.4f} (+{improvement*100:.2f}%)\n",
        "   - Relative Error Reduction: {relative_improvement:.1f}%\n",
        "\n",
        "2. ERROR CORRECTION:\n",
        "   - Baseline Errors: {len(baseline_errors):,}\n",
        "   - Errors Corrected: {len(corrected):,} ({len(corrected)/len(baseline_errors)*100:.1f}%)\n",
        "   - New Errors: {len(new_errors):,} ({len(new_errors)/len(baseline_errors)*100:.1f}%)\n",
        "   - Net Gain: {len(corrected) - len(new_errors):,} errors\n",
        "\n",
        "3. DECISION STATISTICS ({best_strategy.upper()} STRATEGY):\n",
        "   - Agreements: {hybrid_results[best_strategy]['stats']['agreements']:,} ({hybrid_results[best_strategy]['stats']['agreements']/len(X_test)*100:.1f}%)\n",
        "   - Disagreements: {hybrid_results[best_strategy]['stats']['disagreements']:,} ({hybrid_results[best_strategy]['stats']['disagreements']/len(X_test)*100:.1f}%)\n",
        "   - Lexicon Overrides: {hybrid_results[best_strategy]['stats']['lexicon_overrides']:,} ({hybrid_results[best_strategy]['stats']['lexicon_overrides']/len(X_test)*100:.1f}%)\n",
        "   - Override Accuracy: {len(corrected)/(len(corrected)+len(new_errors))*100:.1f}%\n",
        "\n",
        "4. COMPARISON WITH MANUAL ANNOTATION:\n",
        "   From your CAMeLBERT error analysis:\n",
        "   - Total CAMeLBERT errors: 401\n",
        "   - Label noise: 174 (43.4%)\n",
        "   - Real errors: 227 (56.6%)\n",
        "   \n",
        "   Hybrid system addresses:\n",
        "   - Test set errors: {len(baseline_errors):,}\n",
        "   - Corrected: {len(corrected):,} ({len(corrected)/len(baseline_errors)*100:.1f}%)\n",
        "   - Estimated real errors corrected: ~{int(len(corrected) * 0.566):,} (excluding noise)\n",
        "\n",
        "{'='*70}\n",
        "\"\"\"\n",
        "\n",
        "print(paper_results)\n",
        "\n",
        "# Save to file\n",
        "paper_path = f\"{config.HYBRID_RESULTS}/paper_results.txt\"\n",
        "with open(paper_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(paper_results)\n",
        "\n",
        "print(f\"\\nâœ“ Paper results saved to: {paper_path}\")\n",
        "\n",
        "# Also save as markdown\n",
        "md_path = f\"{config.HYBRID_RESULTS}/paper_results.md\"\n",
        "with open(md_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(paper_results)\n",
        "\n",
        "print(f\"âœ“ Markdown version saved to: {md_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENT COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nðŸ“ All results saved to:\")\n",
        "print(f\"   {config.HYBRID_PATH}\")\n",
        "print(\"\\nðŸ“Š Figures:\")\n",
        "print(f\"   {config.HYBRID_FIGURES}\")\n",
        "print(\"\\nðŸ“ˆ Analysis:\")\n",
        "print(f\"   {config.HYBRID_ANALYSIS}\")\n",
        "print(\"\\nâœ… Ready for paper integration!\")"
      ],
      "metadata": {
        "id": "paper_results_cell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
